[["index.html", "Data Preparation and Analysis 1 Experiment Overview", " Data Preparation and Analysis Anna Makova (under supervision of Dr. Jacob Bellmund) 05/08/2021 1 Experiment Overview This experiment was conducted to investigate the development of spatial memory in 8- to 15-year-old children and adolescents. Participants navigated throughout a circular arena with a landmark and extra-maze, orientation cues created in Unity environment. Participants learned positions of 4 objects from which 2 were landmark-bound and 2 were boundary-bound. After block 1, landmark and landmark-dependent objects changed their position and participants had to notice an dlearn new positions. "],["randomisation-script.html", "2 Randomisation script", " 2 Randomisation script library(reticulate) library(here) This script is written in Python #import packages import json import random import math import matplotlib.pyplot as plt from matplotlib.backends.backend_pdf import PdfPages #all definitions def arenaRand(circle_r): # center of the circle (x, y) circle_x = 0 circle_y = 0 # random angle alpha = 2 * math.pi * random.random() # random radius r = circle_r * math.sqrt(random.random()) # calculating coordinates x = r * math.cos(alpha) + circle_x y = r * math.sin(alpha) + circle_y return x, y def ringRand(circle_r1, circle_r2, circle_x = 0, circle_y = 0): # center of the circle (x, y) # random angle alpha = 2 * math.pi * random.random() # random radius r = random.uniform(circle_r1, circle_r2) # calculating coordinates x = r * math.cos(alpha) + circle_x y = r * math.sin(alpha) + circle_y return x, y #distance def def distance(landmark, obj): d = math.sqrt(((landmark[0]-obj[0])**2)+((landmark[2]-obj[2])**2)) return d origin = [0,0,0] codes =[] part = 1 for i in range(1): #number adjusted for the target amount of participants #objList objName = [&quot;none&quot;,&quot;flower&quot;,&quot;slime&quot;,&quot;lamp&quot;,&quot;partyhat&quot;] #learnOrder obj = [1,2,3,4] order = random.sample(obj, k=len(obj)) learnOrder = [0] + order + order #feedbackOrder feedbackOrder = [0] while len(feedbackOrder) &lt; 65: if len(feedbackOrder) == 1: one = random.sample(obj, k=len(obj)) if one[0] != learnOrder[-1]: feedbackOrder = feedbackOrder + one else: feedbackOrder = [0] else: next = random.sample(obj, k=len(obj)) if next[0] != feedbackOrder[-1]: feedbackOrder = feedbackOrder + next #landmarkPos landmarkPos = [[0,0,0]] x, z = ringRand(2,20) y = -2.4 loc = [x, y, z] landmarkPos.append(loc) #familiarisePropLoc familiarisePropLoc = [[0,0,0]] i = 0 while len(familiarisePropLoc) &lt; 11: x, z = arenaRand(22.5) y = 0.25 loc = [x, y, z] #making sure it doesnt land in the landmark position if distance(loc, landmarkPos[1]) &gt; 8 and distance(loc, familiarisePropLoc[i]) &gt; 10: familiarisePropLoc.append(loc) i += 1 #objLoc objLoc = [] #randomisation of objects as landmark-dependent vs boundary-dependent s = random.sample(obj, k=4) landBound1 = s[0] landBound2 = s[1] bounBound1 = s[2] bounBound2 = s[3] #Object Locations in Block 1-4 &amp; Landmark Locations in BLock 2-4 w=1 while len(objLoc) &lt; 4: #setting up objects&#39; locations for Block 1 if len(objLoc) == 0: objLocBlock = [None] * 5 objLocBlock[0] = [0,0,0] y=0.7 while objLocBlock[bounBound1] == None: x1, z1 = ringRand(circle_r1 = 14.5, circle_r2 = 19.5, circle_x = 0, circle_y = 0) if distance([x1,y,z1],landmarkPos[1]) &gt; 8: objLocBlock[bounBound1] = [x1, y, z1] while objLocBlock[bounBound2] == None: x2, z2 = ringRand(circle_r1 = 14.5, circle_r2 = 19.5, circle_x = 0, circle_y = 0) if distance([x2,y,z2],landmarkPos[1]) &gt; 8 and distance([x2,y,z2],objLocBlock[bounBound1]) &gt; 8: objLocBlock[bounBound2] = [x2, y, z2] while objLocBlock[landBound1] == None: x3, z3 = ringRand(circle_r1 = 8, circle_r2 = 13, circle_x = landmarkPos[1][0], circle_y = landmarkPos[1][2]) if distance([x3,y,z3],objLocBlock[bounBound1]) &gt; 8 and distance([x3,y,z3],objLocBlock[bounBound2]) &gt; 8 and distance([x3,y,z3],origin) &lt; 22.5: objLocBlock[landBound1] = [x3, y, z3] while objLocBlock[landBound2] == None: x4, z4 = ringRand(circle_r1 = 8, circle_r2 = 13, circle_x = landmarkPos[1][0], circle_y = landmarkPos[1][2]) if distance([x4,y,z4],objLocBlock[bounBound1]) &gt; 8 and distance([x4,y,z4],objLocBlock[bounBound2]) &gt; 8 and distance([x4,y,z4],objLocBlock[landBound1]) &gt; 8 and distance([x4,y,z4],origin) &lt; 22.5: objLocBlock[landBound2] = [x4, y, z4] objLoc.append(objLocBlock) g = 1 #setting up landmark &amp; objects locations for block 2-4 else: #landmark pos x, z = arenaRand(22.5) y = -2.4 loc = [x, y, z] i = 0 diff = [] for i in range(len(landmarkPos)): #checking distance to other landmark locations if distance(landmarkPos[i], loc) &gt; 10: i += 1 diff.append(&#39;True&#39;) else: i += 1 diff.append(&#39;False&#39;) if &#39;False&#39; not in diff: print(diff) if distance(loc, objLoc[0][bounBound1]) &gt; 8 and distance(loc, objLoc[0][bounBound2]) &gt; 8: landmark = loc i += 1 #location of boundary-dependent objs objLocBlock = [None] * 5 objLocBlock[0] = [0,0,0] objLocBlock[bounBound1] = objLoc[0][bounBound1] objLocBlock[bounBound2] = objLoc[0][bounBound2] print(&#39;boundary-bound locations done&#39;) #location of the landmark-dependent objs x1 = objLoc[0][landBound1][0] + (landmark[0]-landmarkPos[1][0]) y1 = 0.7 z1 = objLoc[0][landBound1][2] + (landmark[2]-landmarkPos[1][2]) locObj1 = [x1, y1, z1] if distance(locObj1, origin) &lt; 22.5 and distance(locObj1, objLoc[0][bounBound1]) &gt; 5 and distance(locObj1, objLoc[0][bounBound2]) &gt; 5: #checking if it is inside the arena print(&#39;inside arena obj1&#39;) objLocBlock[landBound1] = locObj1 x2 = objLoc[0][landBound2][0] + (landmark[0]-landmarkPos[1][0]) y2 = 0.7 z2 = objLoc[0][landBound2][2] + (landmark[2]-landmarkPos[1][2]) locObj2 = [x2, y2, z2] print(&#39;landmark-bound location 1 done&#39;) if distance(locObj2, origin) &lt; 22.5 and distance(locObj2, objLoc[0][bounBound1]) &gt; 5 and distance(locObj2, objLoc[0][bounBound2]) &gt; 5: objLocBlock[landBound2] = locObj2 print(&#39;inside arena obj2&#39;) objLoc.append(objLocBlock) landmarkPos.append(landmark) print(&#39;landmark-bound location 2 done&#39;) print(len(objLoc)) g += 1 else: print(&#39;outside of arena obj2&#39;) else: print(&#39;outside of arena obj1&#39;) else: w += 1 if w &lt; 5000: objLoc = objLoc[0:g] print(&#39;landmark not far away from other landmarks&#39;) landmarkPos = landmarkPos[0:g+1] print(w) else: objLoc = [] landmarkPos = landmarkPos[0:2] w = 0 #visualization landmark_x = [landmarkPos[1][0], landmarkPos[2][0], landmarkPos[3][0], landmarkPos[4][0]] landmark_y = [landmarkPos[1][2], landmarkPos[2][2], landmarkPos[3][2], landmarkPos[4][2]] pp = PdfPages(&#39;input_figs_&#39; + str(part) +&#39;.pdf&#39;) for i in range(4): circle1 = plt.Circle((0,0), 27.5, color=&#39;black&#39;, fill=False) fig, ax = plt.subplots() plt.xlim(-28,28) plt.ylim(-28,28) ax.set_aspect(1) ax.add_artist(circle1) plt.plot(landmark_x[i], landmark_y[i], &#39;xr&#39;) plt.plot(objLoc[i][bounBound1][0], objLoc[i][bounBound1][2], &#39;ob&#39;) plt.plot(objLoc[i][bounBound2][0], objLoc[i][bounBound2][2], &#39;ob&#39;) plt.plot(objLoc[i][landBound1][0], objLoc[i][landBound1][2], &#39;or&#39;) plt.plot(objLoc[i][landBound2][0], objLoc[i][landBound2][2], &#39;or&#39;) plt.title(&#39;Block &#39; + str(i+1), fontsize=8) plt.savefig(pp, format=&#39;pdf&#39;) plt.show() i =+ 1 # creates the overall pdf file with all 4 plots pp.close() #coded ID ID = random.randint(1000,9999) none = [&quot;none&quot;] ID = none + [str(ID)] codes = codes + ID #json file json_data = { &quot;objName&quot; : objName, &quot;learnOrder&quot; : learnOrder, &quot;feedbackOrder&quot; : feedbackOrder, &quot;landmarkPos&quot;:landmarkPos, &quot;familiarisePropLoc&quot;: familiarisePropLoc, &quot;objLoc1&quot;: objLoc[0], &quot;objLoc2&quot;: objLoc[1], &quot;objLoc3&quot;: objLoc[2], &quot;objLoc4&quot;: objLoc[3], &quot;IDnew&quot;: ID } json_dump = json.dumps(json_data) json_input = json.loads(json_dump) #saves the created json to the current folder with open(&#39;psub&#39;+str(part)+&#39;_inputParam.json&#39;,&#39;w&#39;) as json_file: json.dump(json_input, json_file) Block 1 - 4 Objects’ Locations #location restrictions i=2 circle1 = plt.Circle((0,0), 27.5, color=&#39;black&#39;, fill=False) circle2 = plt.Circle((0,0), 19.5, color=&#39;blue&#39;, fill=False, alpha=0.6) circle3 = plt.Circle((0,0), 14.5, color=&#39;blue&#39;, fill=False, alpha=0.6) circle4 = plt.Circle((landmark_x[i], landmark_y[i]), 13, color=&#39;red&#39;, fill=False,alpha=0.6) circle5 = plt.Circle((landmark_x[i], landmark_y[i]), 8, color=&#39;red&#39;, fill=False, alpha=0.6) fig, ax = plt.subplots() plt.xlim(-28,28) plt.ylim(-28,28) ax.set_aspect(1) ax.add_artist(circle1) ax.add_artist(circle2) ax.add_artist(circle3) ax.add_artist(circle4) ax.add_artist(circle5) plt.plot(landmark_x[i], landmark_y[i], &#39;xr&#39;) plt.plot(objLoc[i][bounBound1][0], objLoc[i][bounBound1][2], &#39;ob&#39;) plt.plot(objLoc[i][bounBound2][0], objLoc[i][bounBound2][2], &#39;ob&#39;) plt.plot(objLoc[i][landBound1][0], objLoc[i][landBound1][2], &#39;or&#39;) plt.plot(objLoc[i][landBound2][0], objLoc[i][landBound2][2], &#39;or&#39;) plt.title(&#39;Block &#39;+str(i+1), fontsize=8) plt.show() Location Restrictions #visualising restrictions for new landmakr position in block 3 i=1 circle1 = plt.Circle((0,0), 27.5, color=&#39;black&#39;, fill=False) #arena circle2 = plt.Circle((0,0), 25, color=&#39;black&#39;, fill=False, alpha=0.3) #where objects can be located circle4 = plt.Circle((landmark_x[i], landmark_y[i]), 9, color=&#39;green&#39;, fill=False, alpha=0.5) circle6 = plt.Circle((landmark_x[0], landmark_y[0]), 9, color=&#39;green&#39;, fill=False, alpha=0.5) #minimal change of landmark circle5 = plt.Circle((landmark_x[i], landmark_y[i]), 5, color=&#39;black&#39;, fill=False, alpha=0.3) #minimal distance of objects from the landmark circle10 = plt.Circle((objLoc[i][bounBound1][0], objLoc[i][bounBound1][2]), 5, color=&#39;green&#39;, fill=False, alpha=0.5) #landmark cannot be inside these circles in the next blocks circle11 = plt.Circle((objLoc[i][bounBound2][0], objLoc[i][bounBound2][2]), 5, color=&#39;green&#39;, fill=False, alpha=0.5) fig, ax = plt.subplots() plt.xlim(-30,30) plt.ylim(-30,30) ax.set_aspect(1) ax.add_artist(circle1) ax.add_artist(circle2) ax.add_artist(circle4) ax.add_artist(circle5) ax.add_artist(circle6) ax.add_artist(circle10) ax.add_artist(circle11) plt.plot(landmark_x[i], landmark_y[i], &#39;xr&#39;) plt.plot(landmark_x[2], landmark_y[2], &#39;xg&#39;) plt.plot(objLoc[i][bounBound1][0], objLoc[i][bounBound1][2], &#39;ob&#39;) plt.plot(objLoc[i][bounBound2][0], objLoc[i][bounBound2][2], &#39;ob&#39;) plt.plot(objLoc[i][landBound1][0], objLoc[i][landBound1][2], &#39;or&#39;) plt.plot(objLoc[i][landBound2][0], objLoc[i][landBound2][2], &#39;or&#39;) plt.title(&#39;Block &#39;+str(i+1), fontsize=8) plt.show() Landmark Movement Restrictions #visualizing the feedback ratios circle1 = plt.Circle((0,0), 25, color=&#39;black&#39;, fill=False, alpha=0.3) circle2 = plt.Circle((0,0), 27.5, color=&#39;black&#39;, fill=False) circle3 = plt.Circle((objLoc[1][bounBound1][0], objLoc[1][bounBound1][2]), 3, color=&#39;green&#39;, fill=False, alpha=0.6) circle4 = plt.Circle((objLoc[1][bounBound1][0], objLoc[1][bounBound1][2]), 6, color=&#39;yellow&#39;, fill=False, alpha=0.6) circle5 = plt.Circle((objLoc[1][bounBound1][0], objLoc[1][bounBound1][2]), 9, color=&#39;orange&#39;, fill=False, alpha=0.6) circle6 = plt.Circle((objLoc[1][bounBound1][0], objLoc[1][bounBound1][2]), 12, color=&#39;red&#39;, fill=False, alpha=0.6) fig, ax = plt.subplots() plt.xlim(-28,28) plt.ylim(-28,28) ax.set_aspect(1) ax.add_artist(circle1) ax.add_artist(circle2) ax.add_artist(circle3) ax.add_artist(circle4) ax.add_artist(circle5) ax.add_artist(circle6) plt.plot(landmark_x[i], landmark_y[i], &#39;xr&#39;) plt.plot(objLoc[1][bounBound1][0], objLoc[1][bounBound1][2], &#39;ob&#39;) plt.plot(objLoc[1][bounBound2][0], objLoc[1][bounBound2][2], &#39;ob&#39;) plt.plot(objLoc[1][landBound1][0], objLoc[1][landBound1][2], &#39;or&#39;) plt.plot(objLoc[1][landBound2][0], objLoc[1][landBound2][2], &#39;or&#39;) plt.title(&#39;Block &#39;+str(i+1), fontsize=8) plt.show() Feedback Ratios "],["data-preparation.html", "3 Data Preparation 3.1 Setup 3.2 Summary and Angle files 3.3 Trajectory files 3.4 Memory scores 3.5 Saving the compiled dataset", " 3 Data Preparation 3.1 Setup Packages used for data preparation library(tidyverse) library(here) Functions writted for this analysis #function angleDiff() takes in 2 angles in degrees and gives out the difference between them (0-180) with the proper direction #(i.e. positive = clockwise, negative = counterclockwise; angle1 being theoretical 0/trueAngle) angleDiff &lt;- function(angle1, angle2){ x = angle2 - angle1 if (x &gt; 180) {x = x -360} else if (x &lt; -180) {x = x + 360} return(x) } #function dist() calculates distance between 2 given points (loc1, loc2) while taking in their X and Y separately dist &lt;- function(loc1X, loc1Y, loc2X, loc2Y) { d = sqrt(((loc1X-loc2X)**2)+((loc1Y-loc2Y)**2)) return(d) } #function angle() takes in 2 locations (a, b) and calculates angle between A-B and A-C (created inside the function) #a(X, Y) = character position, b(X, Y) = imaginary object position #necessary for relative influence calculation angle &lt;- function(a_X, a_Y, b_X, b_Y) { c_X = a_X c_Y = a_Y + 10 c = dist(a_X, a_Y, b_X, b_Y) b = dist(a_X, a_Y, c_X, c_Y) a = dist(b_X, b_Y, c_X, c_Y) cos_A = (b^2 + c^2 - a^2) / (2*b*c) angleRad = acos(cos_A) angle = (180 * angleRad) / pi if (a_X &gt; b_X) {angle = 360 - angle} return(angle) } Setting up condition of running the chunks below data_here &lt;- nchar(here()) &lt; 50 Creating a list of subject IDs based on presence of summary files for Block 4 Subsequently, creating list of new random IDs which were used during a transfer of data between computers fn &lt;- list.files(path = here(&quot;data&quot;), pattern=&quot;*_feedbackPhase_block4_Sum&quot;) subjects &lt;- c() for (i_file in length(fn)) { subjects &lt;- substr(fn,1,4) } newID &lt;- sample(c(1000:9999), size=length(subjects), replace=FALSE) Retrieving participants’ age from participants.csv (used for participation tracking purposes, created manually) age &lt;- read.table(here(&quot;data&quot;, &quot;participants.csv&quot;), sep = &quot;;&quot;, header = TRUE, colClasses = &quot;character&quot;) %&gt;% select(study.ID, age, study) %&gt;% filter(study == &quot;yes&quot;) 3.2 Summary and Angle files The files are loaded in and joined into a big data frame containing all information from both summary and angle files for all participants Dataframes created along the way: Sum_all: all participants Sum: all data for 1 participant sub_dat: data from 1 block for 1 participant blocks &lt;- c(1, 2, 3, 4) #number of blocks in the experiment Sum_all = tibble() #creating an empty tibble where eventually will the full dataset be stored for(i_sub in subjects){ #new, empty tibble for each participant Sum = tibble() for (i_block in blocks){ #loading in summary data fn &lt;- dir(path = here(&quot;data&quot;), pattern = sprintf(&quot;%s_feedbackPhase_block%s_Sum&quot;, i_sub, i_block), full.names = TRUE) sub_dat &lt;- as_tibble(read_delim(fn, delim = &quot;\\t&quot;, col_names = FALSE, col_types = &quot;dcdddddddddddddd&quot;)) %&gt;% rename(trial = X1, object = X2, sec2Beg = X3, landmarkX = X4, landmarkZ = X5, landmarkY = X6, objX = X7, objZ = X8, objY = X9, error=X13, secTrialRepl = X14, trialLen = X15, sec2End = X16, remLocX = X10, remLocY = X12, remLocZ = X11) %&gt;% mutate(ID=i_sub, newID=newID[which(subjects == i_sub)], age=age$age[which(i_sub==age$study.ID)], block=i_block, dropTime = (sec2Beg + secTrialRepl), miniblock = NA, objectTrial = NA, cue=NA, landmarkCuePosX=NA, landmarkCuePosY=NA, boundaryCuePosX=NA, boundaryCuePosY=NA, relativeInfluence=NA, cueDissonance = NA, angleError=NA, landmarkAngle=NA, boundaryAngle=NA, relativeAngle=NA, cueDissonanceAngle = NA, distanceTo1 = NA, distanceTo2=NA, distanceTo3=NA, averageDist=NA) #loading in angle data fn_A &lt;- dir(path = here(&quot;data&quot;), pattern = sprintf(&quot;%s_feedbackPhase_block%s_Angle&quot;, i_sub, i_block), full.names = TRUE) sub_dat_A &lt;- as_tibble(read_delim(fn_A, delim = &quot;\\t&quot;, col_names = FALSE, col_types = &quot;dcdddddddddd&quot;)) %&gt;% mutate(ID=i_sub, block=i_block) %&gt;% rename(trial = X1, object = X2, charX = X3, charZ = X4, charY = X5, estAngle = X6, objX = X7, objZ = X8, objY = X9, trueAngle = X10, secTrialEst = X11, sec2Est = X12) #joining together sub_dat and sub_datA sub_dat &lt;- inner_join(sub_dat, sub_dat_A, by=c(&quot;trial&quot;, &quot;object&quot;, &quot;objX&quot;, &quot;objZ&quot;, &quot;objY&quot;, &quot;ID&quot;, &quot;block&quot;)) #renaming objects in German to English sub_dat[sub_dat == &quot;die Lampe&quot;] &lt;- &quot;lamp&quot; sub_dat[sub_dat == &quot;die Blume&quot;] &lt;- &quot;flower&quot; sub_dat[sub_dat == &quot;der Partyhut&quot;] &lt;- &quot;partyhat&quot; sub_dat[sub_dat == &quot;das Monster&quot;] &lt;- &quot;monster&quot; #assigning miniblocks based on trial number for (i_trial in 1:(length(sub_dat$trial))){ if (sub_dat$trial[i_trial] &lt; 5) { sub_dat$miniblock[i_trial] &lt;- 1 } else if (sub_dat$trial[i_trial] &lt; 9) { sub_dat$miniblock[i_trial] &lt;- 2 } else if (sub_dat$trial[i_trial] &lt; 13) { sub_dat$miniblock[i_trial] &lt;- 3 } else { sub_dat$miniblock[i_trial] &lt;- 4 } #assigning objectTrial (1-16) - miniblock order number throughout the experiment sub_dat$objectTrial[i_trial] &lt;- (sub_dat$miniblock[i_trial]+((sub_dat$block[i_trial]-1)*4)) #finding distance between remembered position and other than target objects objects &lt;- c(&quot;monster&quot;, &quot;partyhat&quot;, &quot;lamp&quot;, &quot;flower&quot;) otherObjects &lt;- objects[objects != sub_dat$object[i_trial]] sub_dat$distanceTo1[i_trial] &lt;- dist(sub_dat$remLocX[i_trial], sub_dat$remLocY[i_trial], sub_dat$objX[sample(which(sub_dat$object == otherObjects[1]),1)], sub_dat$objY[sample(which(sub_dat$object == otherObjects[1]),1)]) sub_dat$distanceTo2[i_trial] &lt;- dist(sub_dat$remLocX[i_trial], sub_dat$remLocY[i_trial], sub_dat$objX[sample(which(sub_dat$object == otherObjects[2]),1)], sub_dat$objY[sample(which(sub_dat$object == otherObjects[2]),1)]) sub_dat$distanceTo3[i_trial] &lt;- dist(sub_dat$remLocX[i_trial], sub_dat$remLocY[i_trial], sub_dat$objX[sample(which(sub_dat$object == otherObjects[3]),1)], sub_dat$objY[sample(which(sub_dat$object == otherObjects[3]),1)]) #averaging the distance to the other 3 objects&#39; locations to get a single value sub_dat$averageDist[i_trial] &lt;- (sub_dat$distanceTo1[i_trial] + sub_dat$distanceTo2[i_trial] + sub_dat$distanceTo3[i_trial])/3 #calculating the angle error sub_dat$angleError[i_trial] &lt;- angleDiff(sub_dat$trueAngle[i_trial], sub_dat$estAngle[i_trial]) } Sum &lt;- bind_rows(sub_dat, Sum) } # JB: START ADAPTED SECTION # find names of boundary objects based on X-coordinate (should repeat for boundary) boundary_objects &lt;- Sum %&gt;% count(object, objX) %&gt;% # count appearances of each object at given location arrange(desc(n)) %&gt;% # sort based on count -&gt; boundary objects are always at same position -&gt; high counts slice(c(1,2)) %&gt;% # top two rows should be boundary objects pull(object) # extract boundary object names # find names of landmark objects based on Y-coordinate (should repeat for boundary) landmark_objects &lt;- Sum %&gt;% count(object, objY) %&gt;% # count appearances of each object at given location arrange(desc(n)) %&gt;% # sort based on count -&gt; boundary objects are always at same position -&gt; high counts tail(-2) %&gt;% # top two rows should be boundary objects, exclude them distinct(object) %&gt;% # store only unique object names pull(object) # extract boundary object names #assigning boundary- vs landmark-bound Sum &lt;- Sum %&gt;% mutate(cue = case_when( object %in% boundary_objects ~ &quot;boundary&quot;, object %in% landmark_objects ~ &quot;landmark&quot;) ) # some sanity checks if (length(landmark_objects) != 2){stop(sprintf(&quot;Did not find exactly 2 landmark objects for subject %s&quot;, i_sub))} if (length(boundary_objects) != 2){stop(sprintf(&quot;Did not find exactly 2 boundary objects for subject %s&quot;, i_sub))} if(any(is.na(Sum$cue))){stop(sprintf(&quot;Did not assign cue for at least one trial for subject %s&quot;, i_sub))} # Check that assignment of landmark or boundary produced the same cue value for each block -&gt; there should be 4 rows, one per object if (Sum %&gt;% count(object, cue) %&gt;% nrow() != 4){ warning(sprintf(&quot;Assignment of landmark/boundary cue has gone wrong for subject %s&quot;, i_sub)) } # JB: END ADAPTED SECTION #assigning predicted position based on the cue (boundary vs landmark) for (i_trial in 1:nrow(Sum)) { #block 1 if (Sum$block[i_trial] == 1){ #not applicable for block 1 as there has not been any movement of the landmark -&gt; all true locations and angles Sum$landmarkCuePosX[i_trial] &lt;- Sum$objX[i_trial] Sum$landmarkCuePosY[i_trial] &lt;- Sum$objY[i_trial] Sum$boundaryCuePosX[i_trial] &lt;- Sum$objX[i_trial] Sum$boundaryCuePosY[i_trial] &lt;- Sum$objY[i_trial] Sum$landmarkAngle[i_trial] &lt;- Sum$trueAngle[i_trial] Sum$boundaryAngle[i_trial] &lt;- Sum$trueAngle[i_trial] #block 2 }else if (Sum$block[i_trial] == 2){ #landmark-dependent objects if (Sum$cue[i_trial] == &quot;landmark&quot;) { #position predicted by landmark is the true object position Sum$landmarkCuePosX[i_trial] &lt;- Sum$objX[i_trial] Sum$landmarkCuePosY[i_trial] &lt;- Sum$objY[i_trial] #position predicted by boundary is the true object position in block 1 Sum$boundaryCuePosX[i_trial] &lt;- Sum$objX[sample(which(Sum$block == 1 &amp; Sum$object == Sum$object[i_trial]), 1)] Sum$boundaryCuePosY[i_trial] &lt;- Sum$objY[sample(which(Sum$block == 1 &amp; Sum$object == Sum$object[i_trial]), 1)] #angle predicted by landmark is angle towards the true location Sum$landmarkAngle[i_trial] &lt;- Sum$trueAngle[i_trial] #angle predicted by boundary is angle towards object position in block 1 Sum$boundaryAngle[i_trial] &lt;- angle(Sum$charX[i_trial], Sum$charY[i_trial], Sum$boundaryCuePosX[i_trial], Sum$boundaryCuePosY[i_trial]) #boundary-dependent objects } else { #position predicted by boundary is the true object position Sum$boundaryCuePosX[i_trial] &lt;- Sum$objX[i_trial] Sum$boundaryCuePosY[i_trial] &lt;- Sum$objY[i_trial] #position predicted by landmark is calculated based on landmark movement idx &lt;- which(Sum$objectTrial == 1 &amp; Sum$object == Sum$object[i_trial]) Sum$landmarkCuePosX[i_trial] &lt;- (Sum$objX[idx] + (Sum$landmarkX[i_trial]-Sum$landmarkX[idx])) Sum$landmarkCuePosY[i_trial] &lt;- (Sum$objY[idx] + (Sum$landmarkY[i_trial]-Sum$landmarkY[idx])) #angle predicted by boundary is angle towards the true location Sum$boundaryAngle[i_trial] &lt;- Sum$trueAngle[i_trial] #angle predicted by landmark is angle towards the position predicted by landmark Sum$landmarkAngle[i_trial] &lt;- angle(Sum$charX[i_trial], Sum$charY[i_trial] ,Sum$landmarkCuePosX[i_trial], Sum$landmarkCuePosY[i_trial]) } #block 3 and 4 }else{ #landmark-dependent objects if (Sum$cue[i_trial] == &quot;landmark&quot;) { # true position and angle Sum$landmarkCuePosX[i_trial] &lt;- Sum$objX[i_trial] Sum$landmarkCuePosY[i_trial] &lt;- Sum$objY[i_trial] Sum$landmarkAngle[i_trial] &lt;- Sum$trueAngle[i_trial] #Retrieving coordinates of the object in block 1 block1BoundaryX &lt;- Sum$objX[sample(which(Sum$block == 1 &amp; Sum$object == Sum$object[i_trial]), 1)] block1BoundaryY &lt;- Sum$objY[sample(which(Sum$block == 1 &amp; Sum$object == Sum$object[i_trial]), 1)] #Retrieving coordinates of the object in previous block block2BoundaryX &lt;- Sum$objX[sample(which(Sum$block == (Sum$block[i_trial]-1) &amp; Sum$object == Sum$object[i_trial]), 1)] block2BoundaryY &lt;- Sum$objY[sample(which(Sum$block == (Sum$block[i_trial]-1) &amp; Sum$object == Sum$object[i_trial]), 1)] #Calculating the distance to the remembered position of the 2 retrieved positions diffBlock1 &lt;- dist(block1BoundaryX, block1BoundaryY, Sum$remLocX[i_trial], Sum$remLocY[i_trial]) diffBlock2 &lt;- dist(block2BoundaryX, block2BoundaryY, Sum$remLocX[i_trial], Sum$remLocY[i_trial]) #The position that is closer to the remembered position is assigned as the position predicted by boundary if (diffBlock1 &lt; diffBlock2) { Sum$boundaryCuePosX[i_trial] &lt;- block1BoundaryX Sum$boundaryCuePosY[i_trial] &lt;- block1BoundaryY } else { Sum$boundaryCuePosX[i_trial] &lt;- block2BoundaryX Sum$boundaryCuePosY[i_trial] &lt;- block2BoundaryY } #Calculating the angle predicted by boundary based on the position predicted by boundary Sum$boundaryAngle[i_trial] &lt;- angle(Sum$charX[i_trial], Sum$charY[i_trial], Sum$boundaryCuePosX[i_trial], Sum$boundaryCuePosY[i_trial]) #boundary-dependent objects } else { #true position and angle Sum$boundaryCuePosX[i_trial] &lt;- Sum$objX[i_trial] Sum$boundaryCuePosY[i_trial] &lt;- Sum$objY[i_trial] Sum$boundaryAngle[i_trial] &lt;- Sum$trueAngle[i_trial] #Calculating landmark predicted position as if the object was landmark-dependent since block 1 idx1 &lt;- sample(which(Sum$block == 1 &amp; Sum$object == Sum$object[i_trial]), 1) block1LandmarkX &lt;- (Sum$objX[idx1] + (Sum$landmarkX[i_trial]-Sum$landmarkX[idx1])) block1LandmarkY &lt;- (Sum$objY[idx1] + (Sum$landmarkY[i_trial]-Sum$landmarkY[idx1])) #Calculating new position as if the object was landmark-dependent since previous block idx2 &lt;- sample(which(Sum$block == (Sum$block[i_trial]-1) &amp; Sum$object == Sum$object[i_trial]), 1) block2LandmarkX &lt;- (Sum$objX[idx2] + (Sum$landmarkX[i_trial]-Sum$landmarkX[idx2])) block2LandmarkY &lt;- (Sum$objY[idx2] + (Sum$landmarkY[i_trial]-Sum$landmarkY[idx2])) #Calculating the distance to the remembered position of the 2 calculated positions diffBlock1 &lt;- dist(block1LandmarkX, block1LandmarkY, Sum$remLocX[i_trial], Sum$remLocY[i_trial]) diffBlock2 &lt;- dist(block2LandmarkX, block2LandmarkY, Sum$remLocX[i_trial], Sum$remLocY[i_trial]) #The position that is closer to the remembered position is assigned as the position predicted by landmark if (diffBlock1 &lt; diffBlock2) { Sum$landmarkCuePosX[i_trial] &lt;- block1LandmarkX Sum$landmarkCuePosY[i_trial] &lt;- block1LandmarkY } else { Sum$landmarkCuePosX[i_trial] &lt;- block2LandmarkX Sum$landmarkCuePosY[i_trial] &lt;- block2LandmarkY } #Calculating the angle predicted by landmark based on the position predicted by landmark Sum$landmarkAngle[i_trial] &lt;- angle(Sum$charX[i_trial], Sum$charY[i_trial], Sum$landmarkCuePosX[i_trial], Sum$landmarkCuePosY[i_trial]) } } #calculating the relative influence of each cue on the remembered location of the cued object distL &lt;- dist(Sum$landmarkCuePosX[i_trial], Sum$landmarkCuePosY[i_trial], Sum$remLocX[i_trial], Sum$remLocY[i_trial]) distB &lt;- dist(Sum$boundaryCuePosX[i_trial], Sum$boundaryCuePosY[i_trial], Sum$remLocX[i_trial], Sum$remLocY[i_trial]) Sum$relativeInfluence[i_trial] &lt;- distL / (distL + distB) #calculating the relative influence of each cue on the indicated angle errorL &lt;- abs(angleDiff(Sum$landmarkAngle[i_trial], Sum$estAngle[i_trial])) errorB &lt;- abs(angleDiff(Sum$boundaryAngle[i_trial], Sum$estAngle[i_trial])) Sum$relativeAngle[i_trial] &lt;- errorL / (errorL + errorB) #calculating the relative influence with a new formula &quot;distCorrect/(distCorrect+distOther)&quot; and creating values in the same direction for both cues, new variable name - cue dissonance if (Sum$cue[i_trial] == &quot;boundary&quot;) { Sum$cueDissonance[i_trial] &lt;- distB / (distB + distL) Sum$cueDissonanceAngle[i_trial] &lt;- errorB / (errorL + errorB) } else { Sum$cueDissonance[i_trial] &lt;- distL / (distL + distB) Sum$cueDissonanceAngle[i_trial] &lt;- errorL / (errorL + errorB) } } Sum_all &lt;- bind_rows(Sum_all, Sum) #join the new participant data to the existing overall data frame } Last changes to the dataframe to make it ready for analysis. #assigning the correct format to values in columns used for analysis Sum_all &lt;- Sum_all %&gt;% arrange(ID, objectTrial) %&gt;% #ordering the dataframe according to ID and the experiment sequence mutate( cueMM = recode(cue, &quot;boundary&quot; = 1, &quot;landmark&quot; = -1), #recoding cues to 1 and -1, used for mixed model analysis newID = as.factor(newID), cue = as.factor(cue), age = as.numeric(age) ) # calculating centered age and miniblock (using this more complicated way as opposed to directly do scale() because different subjects can have a different number of rows in the Sum_all tibble) Age_c &lt;- Sum_all %&gt;% group_by(ID) %&gt;% distinct(age) %&gt;% ungroup() %&gt;% mutate(age_c = scale(age, center=TRUE, scale=FALSE)) Mini_c &lt;- Sum_all %&gt;% group_by(ID) %&gt;% distinct(miniblock) %&gt;% ungroup() %&gt;% mutate(mini = scale(miniblock, center=TRUE, scale=FALSE)) Sum_all &lt;- inner_join(Sum_all, Age_c%&gt;%select(ID, age_c), by=&quot;ID&quot;) Sum_all &lt;- inner_join(Sum_all, Mini_c, by=c(&quot;ID&quot;, &quot;miniblock&quot;)) Assigning landmark-dependent/boundary-dependent object 1-2 (currently not used any further) Sum_all &lt;- mutate(Sum_all, cue1=NA) #creating new column for (i_trial in 1:nrow(Sum_all)) { #if this is the first time participant sees the object if (Sum_all$objectTrial[i_trial]==1) { #creates a dataframe &quot;this&quot; that has 2 lines (1 line = current object/trial + 1 line = other landmark-/boundary-dependent object) this &lt;- filter(Sum_all, ID == Sum_all$ID[i_trial] &amp; cue == Sum_all$cue[i_trial] &amp; objectTrial == 1) %&gt;% select(ID, objectTrial, cue, cue1) #if the first object has not been assigned number yet, then this object/trial is assigned 1, otherwise 2 if (is.na(this$cue1[1])) {Sum_all$cue1[i_trial] &lt;- 1} else {Sum_all$cue1[i_trial] &lt;- 2} #retrieves and assigns information about object number from objectTrial 1 } else { Sum_all$cue1[i_trial] &lt;- Sum_all$cue1[which(Sum_all$ID == Sum_all$ID[i_trial] &amp; Sum_all$object == Sum_all$object[i_trial] &amp; Sum_all$objectTrial == 1)] } } 3.3 Trajectory files Loading in Trajectory files and creating one huge data frame (~30,000 lines per participant) Traj = tibble() #creating an empty where eventually the full dataframe will be stored (&gt;2M lines) for(i_sub in subjects){ for (i_block in blocks){ #loading in a single trajectory file fn &lt;- dir(path = here(&quot;data&quot;), pattern = sprintf(&quot;%s_feedbackPhase_block%s_Traj&quot;, i_sub, i_block),full.names=TRUE) sub_dat &lt;- as_tibble(read_delim(fn, delim = &quot;\\t&quot;, col_names = FALSE, col_types = &quot;ddddddddddd&quot;)) %&gt;% mutate(ID=i_sub, newID = NA, block=i_block, miniblock = NA, object = NA, cue=NA) %&gt;% rename(trial=X1, sec2Frame=X2, charX=X3, charZ=X4, charY=X5, rotQua1=X6, rotQua2=X7, rotQua3=X8, rotQua4=X9, rotAngle=X10) %&gt;% relocate(ID, newID, block) #assigning miniblock for (i in 1:nrow(sub_dat)){ if (sub_dat$trial[i] &lt; 5) { sub_dat$miniblock[i] &lt;- 1 } else if (sub_dat$trial[i] &lt; 9) { sub_dat$miniblock[i] &lt;- 2 } else if (sub_dat$trial[i] &lt; 13) { sub_dat$miniblock[i] &lt;- 3 } else { sub_dat$miniblock[i] &lt;- 4 } #sub_dat$objectTrial[i] &lt;- (sub_dat$miniblock[i]+((sub_dat$block[i]-1)*4)) Currently not used so taken out for speed #retrieving information about object and cue from summary file obj &lt;- which(Sum_all$ID == sub_dat$ID[i] &amp; Sum_all$block == sub_dat$block[i] &amp; Sum_all$trial == sub_dat$trial[i]) #some trials in summary files missing therefore it leave the columns as NA is it cannot find the above index if (length(obj) == 1) { sub_dat$object[i] &lt;- Sum_all$object[obj] sub_dat$cue[i] &lt;- Sum_all$cue[obj] #sub_dat$cue1[i] &lt;- Sum_all$cue1[obj] currently not used } } #assigning newID sub_dat$newID &lt;- newID[which(subjects == i_sub)] # append to table with data from all subjects Traj &lt;- bind_rows(sub_dat, Traj) } } Getting rid of trajectory locations outside of the arena which have been probably caused by an internet glitch #removing locations outside of the arena (possibly internet glitch) Traj &lt;- filter(Traj, charX &gt; -27.5 &amp; charX &lt; 27.5 &amp; charY &gt; -27.5 &amp; charY &lt; 27.5) 3.4 Memory scores 3.4.1 Based on randomly distributed 1000 locations within the arena Generating 1000 randomly distributed locations within the arena thousand_x &lt;- c() thousand_y &lt;- c() points &lt;- c(1:1000) circle_r &lt;- 22.5 for (i in points){ # random angle alpha &lt;- 2 * pi * runif(1) # random radius r &lt;- circle_r * sqrt(runif(1)) # calculating coordinates x &lt;- r * cos(alpha) y &lt;- r * sin(alpha) thousand_x &lt;- c(thousand_x, x) thousand_y &lt;- c(thousand_y, y) } Calculating the memory score based on random distribution of 1000 locations within the arena Sum_all &lt;- Sum_all %&gt;% mutate(memoryScoreRand = NA, memoryScoreTraj = NA) #creating 2 new columns in the summary dataframe for (i_trial in 1:nrow(Sum_all)) { farther &lt;- 0 for (i_point in points) { #for each location distance to the true location of the cued object is calculated distance = dist(thousand_x[i_point], thousand_y[i_point], Sum_all$objX[i_trial], Sum_all$objY[i_trial]) #if the distance is bigger than the distance error then 1 is added to the count &quot;farther&quot; if (distance &gt; Sum_all$error[i_trial]) {farther &lt;- farther+1} else {farther &lt;- farther} } #final score is calculating the proportion of the 1000 locations that are farther from the true location than the remembered location Sum_all$memoryScoreRand[i_trial] &lt;- farther/1000 } 3.4.2 Based on 1000 locations taken from the participant’s trajectory Calculating memory scores based on 1000 points taken from the complete trajectory of the participant #basically row counter trials &lt;- 0 for (i_sub in subjects) { #selecting 1000 random locations from participant&#39;s overall trajectory XY &lt;- filter(Traj, ID == i_sub) %&gt;% select(ID, charX, charY) XY &lt;- XY[sample(nrow(XY), 1000), ] for (i_trial in 1:nrow(filter(Sum_all, ID == i_sub))) { farther &lt;- 0 for (i_point in 1:length(points)) { #for each location distance to the true location of the cued object is calculated distance = dist(XY$charX[i_point], XY$charY[i_point], Sum_all$objX[i_trial+trials], Sum_all$objY[i_trial+trials]) #if the distance is bigger than the distance error then 1 is added to the count &quot;farther&quot; if (distance &gt; Sum_all$error[i_trial+trials]) {farther &lt;- (farther+1)} } #final score is calculating the proportion of the 1000 locations that are farther from the true location than the remembered location Sum_all$memoryScoreTraj[i_trial+trials] &lt;- farther/1000 } trials &lt;- trials + nrow(filter(Sum_all, ID == i_sub)) } 3.5 Saving the compiled dataset Last changes before saving the datasets into big files Rearrange the columns into more cohesive order and replacing the local ID with a new ID that is necessary for data transfer. Sum_all &lt;- Sum_all %&gt;% relocate(newID, age, age_c, block, miniblock, mini, objectTrial, trial, cue, object, sec2Beg, dropTime, secTrialRepl, trialLen, sec2End, landmarkX, landmarkY, landmarkZ, objX, objY, objZ, remLocX, remLocY, remLocZ, error, distanceTo1, distanceTo2, distanceTo3, averageDist, memoryScoreRand, memoryScoreTraj, landmarkCuePosX, landmarkCuePosY, boundaryCuePosX, boundaryCuePosY, relativeInfluence, cueDissonance, trueAngle, estAngle, angleError) %&gt;% select(-ID) %&gt;% rename(ID = newID) Replacing local ID with a new ID that is necessary for data transfer, this time in Trajectory dataframe. Traj &lt;- Traj %&gt;% select(-ID) %&gt;% rename(ID = newID) Explanations of all columns: ID - randomly assigned ID of 4 digits age - 8-15 years old (entered as character atm) age_c - centered age block - 1-4 miniblock - 1-4 within block (in each miniblock each object appears once) mini - centered miniblocks objectTrial - 1-16, miniblocks throughout the experiment (in objectTrial 12, participants see a object for 12th time) trial - 1-16 within block cue - landmark vs boundary, randomly assigned in an input files cueMM - i.e. cue fro Mixed Models is a recoded version of cue to -1 and 1 cue1 - 1-2, to get rid of object names (landmark object 1, landmark object 2, boundary object 1, boundary object 2) - currently not generated object - “monster”, “partyhat”, “lamp”, “flower” sec2Beg - seconds from the beginning of the experiment to the beginning to the trial dropTime - seconds from the beginning of the experiment to the indication of remembered location of the cued object secTrialRepl - seconds from the beginning of the trial to the indication of remembered location of the cued object trialLen - trial length in seconds sec2End - seconds from the beginning of the experiment to the end of the trial landmarkX - position of landmark X coordinate landmarkY - position of landmark Y coordinate landmarkZ - position of landmark Z coordinate (not really used/useful) objX - position of the cued object of the trial X coordinate objY - position of the cued object of the trial Y coordinate objZ - position of the cued object of the trial Z coordinate (not really used/useful) remLocX - position participant indicated as the location of the cued object X coordinate remLocY - position participant indicated as the location of the cued object Y coordinate remLocZ - position participant indicated as the location of the cued object Z coordinate error - distance between the remembered location and the true location of the cued object distanceTo1 - distance between the remembered location and a wrong/uncued object 1 distanceTo2 - distance between the remembered location and a wrong/uncued object 2 distanceTo3 - distance between the remembered location and a wrong/uncued object 3 averageDist - average distance between the from remembered location and the other objects memoryScoreRand - memory score calculated based on randomly distributed 10,000 points memoryScoreTraj - memory score calculated based on randomly picked 10,000 points from the participant’s recorded path landmarkCuePosX - position of the cued object based on landmark (as if landmark-dependent) X coordinate, used for relative influence score landmarkCuePosY - position of the cued object based on landmark (as if landmark-dependent) Y coordinate, used for relative influence score boundaryCuePosX - position of the cued object based on boundary (as if boundary-dependent) X coordinate, used for relative influence score boundaryCuePosY - position of the cued object based on boundary (as if boundary-dependent) Y coordinate, used for relative influence score relativeInfluence - relative influence score (distance to landmark-dependent location / (distance to landmark-dependent location + distance to boundary-dependent location)) cueDissonance - distance to the correct location/ (distance to the correct location + distance to location predicted by the other cue) trueAngle - correct angle to the cued object from the current participant’s location estAngle - indicated angle to the cued object from the current participant’s location angleError - difference between trueAngle and estAngle landmarkAngle - angle to a object location predicted by landmark boundaryAngle - angle to a object location predicted by boundary relativeAngle - relative influence score of angle estimation (landmarkAngle/ (landmarkAngle+boundaryAngle)) cueDissonanceAngle - angle to the correct location/ (angle to the correct location + angle to location predicted by the other cue) charX - participant’s position when estimating the angle X coordinate charZ - participant’s position when estimating the angle Z coordinate charY - participant’s position when estimating the angle Y coordinate secTrialEst - seconds from the beginning of the trial to the angle estimation sec2Est - seconds from the beginning of the experiment to the angle estimation Saving these into text files into our local data folder. write.table(Sum_all, file = here(&quot;data&quot;, &quot;Sum.txt&quot;), sep = &quot; &quot;, row.names = FALSE, col.names = TRUE) write.table(Traj, file = here(&quot;data&quot;, &quot;Traj.txt&quot;), sep = &quot; &quot;, row.names = FALSE, col.names = TRUE) "],["analysis.html", "4 Analysis 4.1 Hypothesis 4.2 Set up 4.3 Incomplete datasets 4.4 Descriptives 4.5 Block 1 Performance 4.6 Cue Differences 4.7 Miniblocks Learning 4.8 Age", " 4 Analysis 4.1 Hypothesis Hypothesis: Boundary-dependent object position memory will improve between 8 and 15 years of age, while landmark-dependent object memory will stay relatively constant. 4.2 Set up knitr::opts_chunk$set(echo = TRUE) Packages used for data analysis library(here) library(tidyverse) library(broom) library(lme4) library(sjPlot) library(effsize) library(scico) library(cowplot) library(gghalves) library(ggnewscale) library(ggeffects) library(ggsignif) library(patchwork) library(Cairo) Functions written for this analysis #function circleFun() generates locations of 100 points that are in shape of circle #used for mapping out the arena in graphs circleFun &lt;- function(center = c(0,0), r = 27.5, npoints = 100){ tt &lt;- seq(0,2*pi,length.out = npoints) xx &lt;- center[1] + r * cos(tt) yy &lt;- center[2] + r * sin(tt) return(data.frame(x = xx, y = yy)) } #function dist() calculates distance between given 2 points (loc1, loc2) while taking in their X and Y separately dist &lt;- function(loc1X, loc1Y, loc2X, loc2Y) { d = sqrt(((loc1X-loc2X)**2)+((loc1Y-loc2Y)**2)) return(d) } Following chunk checks whether a folder “figures” exists and potentially creates one if (!dir.exists(here(&quot;figures&quot;))){dir.create(here(&quot;figures&quot;))} Reading in the two big files containing the full dataset Sum_all &lt;- read_delim(here(&quot;data&quot; ,&quot;Sum.txt&quot;), delim = &quot; &quot;, col_names = TRUE, col_types = &quot;fdddddddfdcddddddddddddddddddddddddddddddddddddddd&quot;) Traj &lt;- read_delim(here(&quot;data&quot; ,&quot;Traj.txt&quot;), delim = &quot; &quot;, col_names = TRUE, col_types = &quot;fddddddddddddcd&quot;) Creating a list of subject IDs from the full dataset #generating subject ID list subjects &lt;- unique(Sum_all$ID) Formatting for results output low_p &lt;- &quot;&lt;0.001&quot; 4.2.1 Exclusion criteria Participants are excluded based on performance in block 1 which is quantified using memory scores. Explanation graph Graph below visualizes the calculation of memory score for a single trial. #data sorting i_sub = subjects[5] #randomly selection one participant XY &lt;- filter(Traj, ID == i_sub) %&gt;% select(ID, charX, charY) #filtering the full trajectory of the participant XY &lt;- XY[sample(nrow(XY), 1000), ] #selecting randomly 1000 frames from the participant&#39;s complete trajectory graph_sub &lt;- filter(Sum_all, ID == i_sub) #filtering the summary data of the participant XY &lt;- mutate(XY, distance = NA, outside = NA) #creating 2 new columsn for the trajectory data frame for (i_point in 1:1000){ #calculating the distance between the random trajectory location and the true location of the cued bject in the given trial dista = dist(XY$charX[i_point], XY$charY[i_point], graph_sub$objX[10], graph_sub$objY[10]) XY$distance[i_point] &lt;- dista #adding the distance value to the dataframe #evaluating whether the distance between the random location and the object is bigger or smaller than the distance error in the given trial if (dista &gt; graph_sub$error[10]) {XY$outside[i_point] &lt;- TRUE} else {XY$outside[i_point] &lt;- FALSE} } #final used data outside &lt;- filter(XY, outside == TRUE) #creating separate dataframe for locations that are further from the object than the distance error inside &lt;- filter(XY, outside == FALSE) #creating separate dataframe for locations that are closer to the object than the distance error littleCircle &lt;- circleFun(center=c(graph_sub$objX[10],graph_sub$objY[10] ), r=graph_sub$error[10]) #circle dividing closer/further locations circle &lt;- circleFun() #arena border visualization #graph mem_score &lt;- ggplot(circle, aes(x, y)) + geom_path() + geom_path(subset(Traj, ID %in% i_sub), mapping=aes(x=charX, y=charY), size=0.2, alpha=0.7, linetype = 1) + geom_point(data= outside, aes(x=charX, y=charY, color=&quot;Trajectory Locations Further&quot;), size=0.7, alpha=0.7) + geom_point(data=inside, aes(x=charX, y=charY, color=&quot;Trajectory Locations Closer&quot;), size=0.7) + geom_path(data=littleCircle, aes(x,y), color=&quot;#E58A50&quot;) + geom_point(data=graph_sub, aes(x=objX[10], y=objY[10], color=&quot;True Object Location&quot;), size = 2) + geom_point(data=graph_sub, aes(x=remLocX[10], y=remLocY[10], color=&quot;Remembered Object Location&quot;), size=2) + theme_cowplot() + theme(aspect.ratio=1, axis.title = element_text(size=10), axis.text = element_text(size=10), plot.title = element_text(size=12), plot.subtitle = element_text(size = 11)) + labs(title = &quot;Memory score calculation based on trajectory&quot;, subtitle = paste(&quot;Number of Trajectory Locations Closer / 1000&quot; , paste(&quot;Memory score:&quot;, graph_sub$memoryScoreTraj[10]), sep=&quot;\\n&quot;), x= &#39;X (virtual meters)&#39;, y= &#39;Y (virtual meters)&#39;) + scale_color_manual(name = &quot; &quot;, values= c(&quot;True Object Location&quot; = &quot;#F8DF77&quot;, &quot;Remembered Object Location&quot; = &quot;#95413F&quot;, &quot;Trajectory Locations Closer&quot; = &quot;#E58750&quot;, &quot;Trajectory Locations Further&quot; = &quot;#191900&quot;)) ggsave(filename=&quot;memory_score_visual.pdf&quot;, plot=mem_score, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;memory_score_visual.png&quot;, plot=mem_score, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) mem_score To be included in the analysis, participant’s memory scores from block 1 need to be significantly greater than the chance level 0.5. exclusion &lt;- c() #creating empty list that will contain the subject IDs that will be excluded for (i_sub in subjects) { score &lt;- t.test(subset(Sum_all, ID==i_sub &amp; block==1)$memoryScoreTraj, mu=0.5, alternative = &quot;greater&quot;) %&gt;% tidy() #one-tailed t.test of block 1 memory scores against chance level 0.5 if (score$p.value &gt; 0.05) { exclusion &lt;- c(exclusion, i_sub) Sum_all &lt;- filter(Sum_all, ID != i_sub) } } number of participants excluded: 2 new overall sample size: 36 4.2.2 Timeout sessions Identifying and filtering timeout sessions which are encoded as distance error = -1 (an impossible value) #summary of timeout trials timeout &lt;- filter(Sum_all, error == -1) #filtering timeout trials as they do not include location estimation Sum_all &lt;- filter(Sum_all, error != -1) number of timeout trials: 23 number of participants with timeout trials: 13 average number of timeout trials per participant with timeout trials: 1.769 (sd = ) average number of timeout trials per participant: 0.605 (sd = ) 4.3 Incomplete datasets incomplete &lt;- Sum_all %&gt;% group_by(ID) %&gt;% summarise(n = n()) complete &lt;- length(subset(incomplete, n == 64)$n) incomplete &lt;- length(subset(incomplete, n &lt; 64)$n) complete datasets: 22 incomplete datasets: 14 4.4 Descriptives Age Distribution Graph below shows the age distribution of our participants. #summarising data to get a single line for each participant age &lt;- Sum_all %&gt;% group_by(ID, age) %&gt;% summarise(n = n(), .groups=&quot;drop&quot;) age_graph &lt;- ggplot(age, aes(x=age)) + geom_bar(fill=scico(1, palette=&quot;acton&quot;, begin=0.8)) + theme_cowplot() + background_grid(major=&quot;y&quot;, minor=&quot;y&quot;) + scale_x_continuous(breaks = c(8,9,10,11,12,13,14,15)) + theme(legend.position = &quot;none&quot;, plot.title = element_text(size=12), axis.title = element_text(size=10)) + labs(x=&quot;Age&quot;, y=&quot;Count&quot;, title=&quot;Distribution of age&quot;) #saving the graph as pdf and png ggsave(&quot;age.pdf&quot;, plot=age_graph, units = &quot;cm&quot;, width = 8, height = 6, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;age.png&quot;, plot=age_graph, units = &quot;cm&quot;, width = 8, height = 6, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) age_graph Reaction Time Next, we looked into reaction time (seconds between angle estimation and location estimation). #average RT reaction_time_dat &lt;- Sum_all %&gt;% mutate(posResponse = secTrialRepl - secTrialEst) %&gt;% #calculating the seconds between angle estimation and location estimation group_by(ID, age) %&gt;% summarise(posiMemResponse = mean(posResponse), .groups=&quot;drop&quot;) #raincloud graph showing distribution of RT, 1 point per participant ggplot(reaction_time_dat, aes(x=0, y=posiMemResponse)) + geom_half_violin(aes(x=-0.05), fill=scico(1, palette = &quot;acton&quot;, begin = 0.45), alpha =0.5, color=NA) + geom_point(aes(x=0.105, color=age), position = position_jitter(width =0.05, height = 0), shape=16, size = 2) + scale_color_scico(palette = &quot;acton&quot;) + geom_boxplot(width = .08, outlier.shape = NA) + theme_cowplot() + ylab(&#39;RT&#39;) + xlab(&#39;&#39;) + ggtitle(&#39;Average reaction time for each participant across the entire experiment&#39;) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), aspect.ratio =1, plot.title = element_text(face=&quot;italic&quot;, size=12)) #average RT and sd for the full dataset reaction_time &lt;- summarise(reaction_time_dat, mean = mean(posiMemResponse), sd=sd(posiMemResponse), min=min(posiMemResponse), max=max(posiMemResponse)) reaction_time ## # A tibble: 1 x 4 ## mean sd min max ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.70 3.29 3.66 18.9 The average reaction time was 7.701 seconds (sd = 3.288). Trial Length We also calculated the average length of a single trial. #average trial length for the full dataset trial_length &lt;- Sum_all %&gt;% summarise(mean = mean(trialLen), sd = sd(trialLen), min = min(trialLen), max = max(trialLen)) The average trial length was 27.48 seconds (sd = 12.032). 4.5 Block 1 Performance This subsection of the analysis aims to answer whether the participants completed the basic task according to the instructions before any manipulation was introduced. 4.5.1 Recalled locations First, we wanted to visualize the recalled locations relative to the object location so we calculated a new “error location” as if the cued object was at coordinate 0,0 inside the arena and created a heat map. The boundary of the arena is drawn in white. #creating a smaller data frame for trials only in block 1 and calculating the new error coordinates (newX, newY) heatMapDat &lt;- Sum_all %&gt;% filter(block==1) %&gt;% select(ID, miniblock, objX, objY, remLocX, remLocY, cue, block) %&gt;% mutate(newX = remLocX-objX, newY = remLocY - objY) #graphing heat_map_B1 &lt;- ggplot(heatMapDat, aes(x=newX, y=newY)) + # creating a heat map stat_density_2d(aes(fill = ..density..), geom = &quot;raster&quot;, contour = FALSE, na.rm=TRUE) + # setting a color palette from scico package scale_fill_scico(palette = &#39;lajolla&#39;, begin=1, end=0, name = &quot;Density&quot;) + # adding individual error locations on top in white geom_point(alpha=0.2, color=&quot;white&quot;, size=0.05) + # adding the boundary of the arena (as a reference for distance) geom_path(data = circle, aes(x, y), color=&quot;grey&quot;) + # plotting the x and y axis on 0,0 to visualize the center of the arena geom_hline(yintercept = 0) + geom_vline(xintercept = 0) + #changing titles labs(x = &quot;Error on X (vm)&quot;, y = &quot;Error on Y (vm)&quot;, title=&quot;Centralised Distribution of Error&quot;) + # aesthetical changes theme_cowplot() + theme(aspect.ratio=1, plot.title = element_text(face=&quot;italic&quot;, size=12), legend.title = element_text(size=10), legend.text = element_text(size=10)) + # adjusting axis limits and breaks scale_x_continuous(limits = c(-33, 33), breaks = c(-30, -20, -10, 0, 10, 20, 30)) + scale_y_continuous(limits = c(-33, 33), breaks = c(-30, -20, -10, 0, 10, 20, 30)) #saving the graph as pdf and png ggsave(&quot;heatMap_block1.pdf&quot;, plot=heat_map_B1, units = &quot;cm&quot;, width = 10, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;heatMap_block1.png&quot;, plot=heat_map_B1, units = &quot;cm&quot;, width = 10, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) #show the graph heat_map_B1 4.5.2 Memory score #creating dataset that only contains data from block 1 and summarizing for each participant summaryBlock1 &lt;- Sum_all %&gt;% filter(block==1) %&gt;% group_by(ID, block) %&gt;% summarise(memoryScore = mean(memoryScoreTraj), distanceOther = mean(averageDist), distanceTrue = mean(error), .groups=&quot;drop&quot;) To tested whether the memory scores are above chance level 0.5, we ran one sample, one tail t-test. b1_stats &lt;- t.test(summaryBlock1$memoryScore, mu = 0.5, alternative=&quot;greater&quot;) %&gt;% tidy() d &lt;- cohen.d(d=summaryBlock1$memoryScore, f=NA, mu=0.5) b1_stats$d &lt;- d$estimate b1_stats$dCI_low &lt;- d$conf.int[[1]] b1_stats$dCI_high &lt;- d$conf.int[[2]] Block 1 memory score t-test results: t(35)= 25.55, p&lt;0.001, d=4.26, 95% CI [3.04, 5.48] Raincloud graph below visualizes the distribution of average memory scores in block 1 (1 point per participant) mem_score_B1 &lt;- ggplot(summaryBlock1, aes(x=block, y= memoryScore)) + # violin plot geom_half_violin(aes(x=block-0.06), fill=scico(1, palette = &quot;lajolla&quot;, begin = 0.45), alpha =0.5, color=NA) + # single subject data points (1 per participant) with horizontal jitter geom_point(aes(x=block+0.08), position = position_jitter(width =0.01, height = 0), shape=16, size = 1) + # boxplot of distribution (median, 1st and 3rd quartile) geom_boxplot(width = .05, outlier.shape = NA) + # adding plot of mean and SEM stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(-.06), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(-.06), colour = &quot;black&quot;, width = 0, size = 0.5) + # adding horizontal line at chance level 0.5 with annotation geom_hline(yintercept = 0.5, linetype=2) + annotate(&quot;text&quot;, label=&quot;chance = 0.5&quot;, x=1.2, y=0.52, size=3) + # correcting labels labs(x = &#39;&#39;, y = &#39;Memory Score (0-1)&#39;, title = &#39;Memory score&#39;) + # aesthetical changes theme_cowplot() + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), aspect.ratio = 1, plot.title = element_text(face=&quot;italic&quot;, size=12)) #saving the graph as pdf and png ggsave(&quot;memory_score_B1.pdf&quot;, plot=mem_score_B1, units = &quot;cm&quot;, width = 10, height = 7, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;memory_score_B1.png&quot;, plot=mem_score_B1, units = &quot;cm&quot;, width = 10, height = 7, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) #show the graph mem_score_B1 Assembly of graphs for positional memory in block 1. # using patchwork package to combine 2 separate graphs memory_B1 &lt;- mem_score_B1 + heat_map_B1 &amp; theme(axis.text = element_text(size=10), axis.title = element_text(size=10), plot.title = element_text(size=12)) &amp; plot_annotation(title = &#39;Memory Performance in Block 1&#39;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;))) # saving the compiled graphs as pdf and png ggsave(&quot;Block1_loc.pdf&quot;, plot=memory_B1, units = &quot;cm&quot;, width = 15.9, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;Block1_loc.png&quot;, plot=memory_B1, units = &quot;cm&quot;, width = 15.9, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) # show the compiled graphs memory_B1 4.5.3 Distance from remembered locations to other locations We wanted to check that participants tried to recall the location of the correct object and did not mistake it with other 3 objects on regular basis so we calculated the average distance from the recalled location to the other objects’ locations (distanceToOther1 + distanceToOther2 + distanceToOther3 / 3) and compared it to the distance error to the true location of the cued object. This calculation is visualized in a graph below for a single example trial. #filtering for only trials in 1 miniblock for simplicity graph_sub &lt;- graph_sub %&gt;% filter(objectTrial == 5) distance_other &lt;- ggplot(circle, aes(x, y)) + # creating arena boundary geom_path() + # true location of cued object geom_point(data=graph_sub, aes(x=objX[2], y=objY[2], color=&quot;True Object Location&quot;), size = 2.5) + # line visualizing the distance error (between recalled location and true location) geom_segment(data=graph_sub, aes(x=remLocX[2], y=remLocY[2], xend= objX[2], yend= objY[2], color=&quot;True Object Location&quot;), alpha=.5) + # location of other object 1 &amp; distance to the recalled location geom_point(data=graph_sub, aes(x=remLocX[1], y=remLocY[1], color=&quot;Other Object Location&quot;), size=2.5) + geom_segment(data=graph_sub, aes(x=remLocX[1], y=remLocY[1], xend= remLocX[2], yend= remLocY[2], color=&quot;Other Object Location&quot;), alpha=.5) + # location of other object 2 &amp; distance to the recalled location geom_point(data=graph_sub, aes(x=remLocX[3], y=remLocY[3], color=&quot;Other Object Location&quot;), size=2.5) + geom_segment(data=graph_sub, aes(x=remLocX[3], y=remLocY[3], xend= remLocX[2], yend= remLocY[2], color=&quot;Other Object Location&quot;), alpha=.5) + # location of other object 3 &amp; distance to the recalled location geom_point(data=graph_sub, aes(x=remLocX[4], y=remLocY[4], color=&quot;Other Object Location&quot;), size=2.5) + geom_segment(data=graph_sub, aes(x=remLocX[4], y=remLocY[4], xend= remLocX[2], yend= remLocY[2], color=&quot;Other Object Location&quot;), alpha=.5) + # recalled location of the cued object geom_point(data=graph_sub, aes(x=remLocX[2], y=remLocY[2], color=&quot;Remembered Object Location&quot;), size=2.5) + # aesthetical changes theme_cowplot() + theme(aspect.ratio=1, axis.title = element_text(size=10), axis.text = element_text(size=10), plot.title = element_text(size=12), plot.subtitle = element_text(size = 11)) + # changing labels and title labs(title = &quot;Distance from remembered location to other locations&quot;, subtitle = paste(sprintf(&quot;Distance Error: %s vm&quot;, graph_sub$error[2]), sprintf(&quot;Average Distance to Other Objects: %s vm&quot;, round(graph_sub$averageDist[2], 2)), sep=&quot;\\n&quot;), x= &#39;X (vm)&#39;, y= &#39;Y (vm)&#39;) + # specifying colors manually scale_color_manual(name = &quot; &quot;, values= c(&quot;True Object Location&quot; = &quot;#883E3A&quot;, &quot;Remembered Object Location&quot; = &quot;#E37D50&quot;, &quot;Other Object Location&quot; = &quot;#F6D868&quot;)) # saving graph as pdf and png ggsave(filename=&quot;distance_other_visual.pdf&quot;, plot=distance_other, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;distance_other_visual.png&quot;, plot=distance_other, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) # show the graph distance_other To test this, we ran paired t-test which showed that on average there is a difference between these 2 distances. dist_stats &lt;- t.test(summaryBlock1$distanceOther, summaryBlock1$distanceTrue, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(summaryBlock1$distanceOther, summaryBlock1$distanceTrue, paired = TRUE) dist_stats$d &lt;- d$estimate dist_stats$dCI_low &lt;- d$conf.int[[1]] dist_stats$dCI_high &lt;- d$conf.int[[2]] Block 1 distance error t-test results: t(35)= 15.93, p&lt;0.001, d=3.82, 95% CI [2.44, 5.19] Graph visualizing this comparison is below. #necessary pivoting of the dataset for the following graph summaryBlock1 &lt;- pivot_longer(summaryBlock1, cols=c(distanceTrue, distanceOther)) distance_B1 &lt;- ggplot(summaryBlock1, aes(x=name, y= value)) + # specifying axis limits and breaks scale_x_discrete(limits = c(&#39;distanceTrue&#39;, &#39;distanceOther&#39;), labels = c(&#39;to Correct Location&#39;, &#39;to Other Object Locations&#39;)) + scale_y_continuous(limits = c(0, 28), breaks = c(5, 10, 15, 20 , 25)) + # violin plot gghalves::geom_half_violin(data=summaryBlock1 %&gt;% filter(name==&quot;distanceOther&quot;), position=position_nudge(+0.2), aes(fill=name),alpha =0.7, color=NA, side=&quot;r&quot;) + gghalves::geom_half_violin(data=summaryBlock1 %&gt;% filter(name==&quot;distanceTrue&quot;), position=position_nudge(-0.2), aes(fill=name),alpha =0.7, color=NA, side=&quot;l&quot;) + # scico palette lajolla for violin plot fill scale_fill_scico_d(palette = &#39;lajolla&#39;, begin=0.2, end=0.75) + # single subject data points (1 per participant) geom_point(shape=16, size = 1) + # boxplot of distribution (median, 1st and 3rd quartile) geom_boxplot(data=summaryBlock1 %&gt;% filter(name==&quot;distanceOther&quot;), position=position_nudge(+0.1), width = .1, outlier.shape = NA) + geom_boxplot(data=summaryBlock1 %&gt;% filter(name==&quot;distanceTrue&quot;), position=position_nudge(-0.1), width = .1, outlier.shape = NA) + # adding plot of mean and SEM stat_summary(data=summaryBlock1 %&gt;% filter(name==&quot;distanceOther&quot;), position=position_nudge(+0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=summaryBlock1 %&gt;% filter(name==&quot;distanceOther&quot;), position=position_nudge(+0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + stat_summary(data=summaryBlock1 %&gt;% filter(name==&quot;distanceTrue&quot;), position=position_nudge(-0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=summaryBlock1 %&gt;% filter(name==&quot;distanceTrue&quot;), position=position_nudge(-0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + # line connecting individual participants&#39; values geom_line(aes(group=ID), alpha=0.5) + # changing title and labels labs(x = &quot; &quot;, y = &quot;Distance (vm)&quot;, subtitle = &quot;In Block 1&quot;, title = &quot;Distance from Remembered Location to Object Locations&quot;) + # aesthetical changes theme_cowplot() + theme(legend.position = &quot;none&quot;, plot.title = element_text(size=12, lineheight = 1.1), plot.subtitle = element_text(face = &quot;bold&quot;, lineheight = 1.1), aspect.ratio = 0.55, axis.title = element_text(size=12)) + # visualizing the significance of the t-test geom_signif(comparisons = list(c(&quot;distanceTrue&quot;, &quot;distanceOther&quot;)), test=&quot;t.test&quot;, test.args=list(alternative = &quot;two.sided&quot;, var.equal = FALSE, paired=TRUE), map_signif_level = TRUE, tip_length = 0, extend_line = 0.045, y_position = 26) # saving the graph as pdf and png ggsave(&quot;distances_B1.pdf&quot;, plot=distance_B1, units = &quot;cm&quot;, width = 13, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;distances_B1.png&quot;, plot=distance_B1, units = &quot;cm&quot;, width = 13, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) # show the graph distance_B1 4.6 Cue Differences Next we wanted to investigate whether the participants followed the cues in block 2-4 after landmark and landmark-dependent object movement. Also, is there performance difference between landmark- vs boundary-dependent objects? 4.6.1 Relative Influence Relative Influence has been used in the field for over a decade now, initially introduced in paper by Doeller, King and Burgess (2008). “For blocks 2–4, we attempted to quantify the relative influence of either cue on each response location. In a pilot study, we noticed that incorrect responses tended to be clustered around locations previously associated with the incorrect cue: either during block 1 or during the immediately preceding block. Accordingly, we calculated the relative influence of boundary versus landmark in blocks 2–4 as d L/(d L + d B), where d L is the distance of the response from the location predicted by the landmark and d B is the distance from the location predicted by the boundary. This measure varies between 0 (using the landmark) and 1 (using the boundary). On the basis of our pilot data the incorrect cue potentially predicts two different locations in blocks 3 and 4 (reflecting the object’s positions relative to it in the preceding block and in block 1): we used whichever was closest to the response location.” Explanation Graph Below, the calculation of relative influence score is visualised. #loading a data from block 2-4 for a single participant graph_sub &lt;- filter(Sum_all, ID == subjects[5] &amp; block == 4 &amp; trial==15) relativeInf_graph &lt;- ggplot(data=graph_sub) + # location predicted by landmark geom_point(aes(x=landmarkCuePosX, y=landmarkCuePosY, color=&quot;Location Predicted by Landmark&quot;), size = 2.5) + # line visualizing the distance error (between recalled location and landmark-predicted location) geom_segment(aes(x=remLocX, y=remLocY, xend= landmarkCuePosX, yend=landmarkCuePosY, color=&quot;Location Predicted by Landmark&quot;), alpha=.5) + # adding dL label annotate(&quot;text&quot;, label=&quot;dL&quot;, x=-7.8, y=-9.5, size=4, color=&quot;#C6F1B1&quot;, fontface =2) + # location predicted by boundary geom_point(aes(x=boundaryCuePosX, y=boundaryCuePosY, color=&quot;Location Predicted by Boundary&quot;), size = 2.5) + # line visualizing the distance error (between recalled location and true location) geom_segment(aes(x=boundaryCuePosX, y=boundaryCuePosY, xend= remLocX, yend= remLocY, color=&quot;Location Predicted by Boundary&quot;), alpha=.5) + # adding dB label annotate(&quot;text&quot;, label=&quot;dB&quot;, x=10, y=3, size=4, color=&quot;#592758&quot;, fontface =2) + #recalled location geom_point(aes(x=remLocX, y=remLocY, color = &quot;Remembered Object Location&quot;), size = 2.5) + #landmark location geom_point(aes(x=landmarkX, y=landmarkY), shape = 15, color = &quot;#94A98F&quot;, size=3) + annotate(&quot;text&quot;, label=&quot;Landmark&quot;, x= -16, y = -3.5, size=3.5, color= &quot;#94A98F&quot;) + # creating arena boundary geom_path(data=circle, aes(x, y)) + # aesthetical changes theme_cowplot() + theme(aspect.ratio=1, axis.title = element_text(size=10), axis.text = element_text(size=10), plot.title = element_text(size=12), plot.subtitle = element_text(size = 11)) + # changing labels and title labs(title = &quot;Relative Influence Calculation&quot;, subtitle = paste(&quot;dL / (dL + dB) &quot;, sprintf(&quot;Relative Influence: %s &quot;, round(graph_sub$relativeInfluence, 2)), sep=&quot;\\n&quot;), x= &#39;X (vm)&#39;, y= &#39;Y (vm)&#39;) + # specifying colors manually scale_color_manual(name = &quot; &quot;, values= c(&quot;Location Predicted by Boundary&quot; = &quot;#592758&quot;, &quot;Location Predicted by Landmark&quot; = &quot;#C6F1B1&quot;, &quot;Remembered Object Location&quot; = &quot;red&quot;)) # saving graph as pdf and png ggsave(filename=&quot;relative_influence_visual.pdf&quot;, plot=relativeInf_graph, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;relative_influence_visual.png&quot;, plot=relativeInf_graph, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) # show the graph relativeInf_graph First, let’s create a subset dataframe that has data only from block 2-4 and get rid of us currently unnecessary columns. Then we summarize relative influence score, correct cue influence and distance error. subset_RI &lt;- Sum_all %&gt;% filter(block!=1) %&gt;% select(-object, -sec2Beg, -sec2Est, -dropTime, -sec2End, -trialLen, -secTrialEst) relativeInfluenceBlocks &lt;- subset_RI %&gt;% group_by(ID, cue) %&gt;% summarise(relativeInf = mean(relativeInfluence), cueDis = mean(cueDissonance), distanceError = mean(error), age=unique(age), .groups=&quot;drop&quot;) T-Tests As score 0.5 points towards location between the location predicted by landmark and location predicted by boundary, we ran a one sample, one tail t-test to test whether the relative scores for landmark-dependent objects are less than 0.5. landmarkRI_stats &lt;- t.test(subset(relativeInfluenceBlocks, cue==&quot;landmark&quot;)$relativeInf, mu = 0.5, alternative = &quot;less&quot;) %&gt;% tidy() d &lt;- cohen.d(d=subset(relativeInfluenceBlocks, cue==&quot;landmark&quot;)$relativeInf, f=NA, mu=0.5) landmarkRI_stats$d &lt;- d$estimate landmarkRI_stats$dCI_low &lt;- d$conf.int[[1]] landmarkRI_stats$dCI_high &lt;- d$conf.int[[2]] Landmark-dependent objects’ RI against 0.5 t-test results: t(35)= -8.2, p&lt;0.001, d=-1.37, 95% CI [-2.12, -0.62] After we ran a one sample, one tail t-test to test whether the relative scores for boundary-dependent objects are higher than 0.5. boundaryRI_stats &lt;- t.test(subset(relativeInfluenceBlocks, cue==&quot;boundary&quot;)$relativeInf, mu = 0.5, alternative = &quot;greater&quot;) %&gt;% tidy() d &lt;- cohen.d(d=subset(relativeInfluenceBlocks, cue==&quot;boundary&quot;)$relativeInf, f=NA, mu=0.5) boundaryRI_stats$d &lt;- d$estimate boundaryRI_stats$dCI_low &lt;- d$conf.int[[1]] boundaryRI_stats$dCI_high &lt;- d$conf.int[[2]] Boundary-dependent objects’ RI against 0.5 t-test results: t(35)= 10.22, p&lt;0.001, d=1.7, 95% CI [0.91, 2.49] We also tested these two groups against each other to see if the scores are significantly different. RI_stats &lt;- t.test(subset(relativeInfluenceBlocks, cue==&quot;boundary&quot;)$relativeInf, subset(relativeInfluenceBlocks, cue==&quot;landmark&quot;)$relativeInf, paired=TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(relativeInfluenceBlocks, cue==&quot;boundary&quot;)$relativeInf, subset(relativeInfluenceBlocks, cue==&quot;landmark&quot;)$relativeInf, paired=TRUE) RI_stats$d &lt;- d$estimate RI_stats$dCI_low &lt;- d$conf.int[[1]] RI_stats$dCI_high &lt;- d$conf.int[[2]] Boundary-dependent objects’ RI vs landmark-dependent objects’ RI t-test results: t(35)= 10.65, p&lt;0.001, d=3.06, 95% CI [1.69, 4.43] Graph This difference in relative influence scores between boundary-dependent objects and landmark-dependent objects is visualized in a graph below. rel_Inf &lt;- ggplot(relativeInfluenceBlocks, aes(x=cue, y=relativeInf)) + # changing labels and titles scale_x_discrete(labels = c(&#39;Boundary&#39;, &#39;Landmark&#39;)) + labs(x = &quot; &quot;, y = &quot;Relative Influence (0-1)&quot;, title = &quot;Positional Memory&quot;) + # violin plots gghalves::geom_half_violin(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;r&quot;) + gghalves::geom_half_violin(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;l&quot;) + # scico palette tokyo scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.15, end=0.85) + # single subject data points (1 per participant) geom_point(shape=16, size = 1) + # line connecting individual participants&#39; values geom_line(aes(group=ID), alpha=0.5) + # boxplot of distribution (median, 1st and 3rd quartile) geom_boxplot(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.1), width = .1, outlier.shape = NA) + geom_boxplot(data= relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.1), width = .1, outlier.shape = NA) + # adding plot of mean and SEM stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + # aesthetical changes theme_cowplot() + theme(legend.position = &quot;none&quot;, plot.title = element_text(size=12, lineheight = 1.1), axis.title = element_text(size=10)) + # visualizing the significance level of the t-test geom_signif(comparisons = list(c(&quot;landmark&quot;, &quot;boundary&quot;)), test=&quot;t.test&quot;, test.args=list(alternative = &quot;two.sided&quot;, paired=TRUE), map_signif_level = TRUE, tip_length = 0, extend_line = 0.045, y_position = 0.83) + # adding line at 0.5 (not-following either cue) geom_hline(yintercept=0.5, linetype=2, alpha=0.6) + # changing limits and breaks of y-axis scale_y_continuous(limits=c(0.1, 0.9), breaks = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)) + # adding annotation of significance level of individual t-tests annotate(&quot;text&quot;, label=&quot;***&quot;, size=3, y=0.8, x=0.9, fontface =2) + annotate(&quot;text&quot;, label=&quot;***&quot;, size=3, y=0.15, x=2.1, fontface =2) # saving graphs as pdf and png ggsave(&quot;relativeInfluence.pdf&quot;, plot=rel_Inf, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;relativeInfluence.png&quot;, plot=rel_Inf, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) # show the graph rel_Inf Mixed Models As a first step towards building a full mixed effects model, we tested a simpliest version with only cue as a fixed effect and a random slope and random intercepts for participants. We did not use cue as a factor but in a recoded version of landmark = -1 and boundary = 1. formulaCue &lt;- &quot;relativeInfluence ~ cueMM + (1+cueMM|ID)&quot; modelCue &lt;- lme4::lmer(formula = formulaCue, data=subset_RI) tab_model(modelCue, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.5108 0.0070 0.4970 – 0.5246 Cue 0.1270 0.0119 0.1036 – 0.1504 Random Effects σ2 0.0407 τ00 ID 0.0009 τ11 ID.cueMM 0.0043 ρ01 ID -0.0876 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.260 / 0.344 To test, whether cue is a significant predictor we ran a likelihood ratio test comparing our simpliest model and a model containing only random effects. formulaCueControl&lt;- &quot;relativeInfluence ~ 1 + (1+cueMM|ID)&quot; modelCueControl &lt;- lme4::lmer(formula = formulaCueControl, data=subset_RI) ratioCue &lt;- anova(modelCue, modelCueControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 51.989, p&lt;0.001 4.6.2 Distance Error Differences Previously, Julian et al (2019) showed that there is a difference in distance error between boundary-dependent and landmark-dependent objects so we ran a paired t-test to replicate these findings in this new participant demographic. distCue_stats &lt;- t.test(subset(relativeInfluenceBlocks, cue==&quot;boundary&quot;)$distanceError, subset(relativeInfluenceBlocks, cue==&quot;landmark&quot;)$distanceError, paired=TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(relativeInfluenceBlocks, cue==&quot;boundary&quot;)$distanceError, subset(relativeInfluenceBlocks, cue==&quot;landmark&quot;)$distanceError, paired=TRUE) distCue_stats$d &lt;- d$estimate distCue_stats$dCI_low &lt;- d$conf.int[[1]] distCue_stats$dCI_high &lt;- d$conf.int[[2]] Boundary-dependent objects’ distance error vs landmark-dependent objects’ distance error t-test results: t(35)= 1.61, p= 0.117, d=0.21, 95% CI [-0.05, 0.46] We did not replicate these findings and the lack of difference is evident in the graph below. cue_Dist &lt;- ggplot(relativeInfluenceBlocks, aes(x=cue, y= distanceError)) + # changing labels and title scale_x_discrete(labels = c(&#39;Boundary&#39;, &#39;Landmark&#39;)) + labs(x = &quot; &quot;, y = &quot;Distance Error (vm)&quot;, title = &quot;Positional Memory&quot;) + # violin plot gghalves::geom_half_violin(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;r&quot;) + gghalves::geom_half_violin(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;l&quot;) + # scico palette tokyo scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.15, end=0.85) + # single subject data points (1 per participant) geom_point(shape=16, size = 1) + # line connecting individual participants&#39; values geom_line(aes(group=ID), alpha=0.5) + # boxplot of distribution (median, 1st and 3rd quartile) geom_boxplot(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.1), width = .1, outlier.shape = NA) + geom_boxplot(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.1), width = .1, outlier.shape = NA) + # adding plot of mean and SEM stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeInfluenceBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + # aesthetical changes theme_cowplot() + theme(legend.position = &quot;none&quot;, plot.title = element_text(size=12, lineheight = 1.1), axis.title = element_text(size=12)) + # visualizing the significance level of the t-test geom_signif(comparisons = list(c(&quot;landmark&quot;, &quot;boundary&quot;)), test=&quot;t.test&quot;, test.args=list(alternative = &quot;two.sided&quot;, paired=TRUE), map_signif_level = TRUE, tip_length = 0, extend_line = 0.045, y_position = 20.1, textsize=2.75) # saving the graph as pdf and png ggsave(&quot;distancesError.pdf&quot;, plot=cue_Dist, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;distancesError.png&quot;, plot=cue_Dist, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) # show the plot cue_Dist 4.7 Miniblocks Learning 4.7.1 Graphs The graph below shows the relative influence scores averaged for each miniblock in block 2-4 (averaging over 6 scores for each miniblock) # necessary summarizing for the graph below summary_miniblock &lt;- subset_RI %&gt;% group_by(ID, miniblock, cue) %&gt;% summarise(relativeInf = mean(relativeInfluence), riSD = sd(relativeInfluence), cueDis = mean(cueDissonance), cdSD = sd(cueDissonance), .groups = &quot;drop&quot;) g_mini &lt;- ggplot(summary_miniblock, aes(miniblock, relativeInf, group=interaction(cue, ID), color = cue)) + # assigning scico palette tokyo as a color palette scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.75, end=0.2, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;)) + # connects the mean values for each participant geom_line(size=0.5, alpha = 0.3) + # overall mean and se stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size=1) + stat_summary(fun.data=mean_se, aes(group=cue), geom=&quot;errorbar&quot;, width=0.1, alpha=0.95) + # connects the overall mean to show the improvement stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + # line showing 0.5 neutral relative score for reference geom_hline(yintercept = 0.5, linetype = 2) + # changing labels and titles labs(x= &quot;Miniblock&quot;, y = &quot;Relative Influence (0-1)&quot;, subtitle = &quot;Averaged across \\nBlocks&quot;, title=&quot; &quot;, color = &quot;Cue&quot;) + # aesthitical changes theme_cowplot() + # changing limits and breaks on both axis scale_y_continuous(limits = c(0,1), breaks = c(0, 0.25, 0.5, 0.75, 1)) + scale_x_continuous(limits = c(0.5, 4.5), breaks = c(1,2,3,4)) + theme(axis.title.y = element_blank(), legend.position = &quot;none&quot;) #show the graph g_mini The graph below shows the relative influence scores for each miniblock throughout block 2-4 (averaging only over 2 scores per miniblock) #necessary summarizing for the graph below summary_objectTrial &lt;- subset_RI %&gt;% group_by(ID, objectTrial, cue) %&gt;% summarise(relativeInf = mean(relativeInfluence), riSD = sd(relativeInfluence), .groups = &quot;drop&quot;) g_objTrial &lt;- ggplot(summary_objectTrial, aes(objectTrial, relativeInf, group=interaction(cue, ID), color=cue)) + # connects the mean values for each participant within each block geom_line(data= subset(summary_objectTrial, objectTrial &lt; 9), size=0.5, alpha = 0.3) + geom_line(data= subset(summary_objectTrial, objectTrial &lt; 13 &amp; objectTrial &gt; 8), size=0.5, alpha = 0.3) + geom_line(data= subset(summary_objectTrial, objectTrial &gt; 12), size=0.5, alpha = 0.3) + # overall mean and se stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size=1) + stat_summary(fun.data=mean_se, aes(group=cue), geom=&quot;errorbar&quot;, width=0.1, alpha=0.95) + # line connecting the average values within each block stat_summary(data = subset(summary_objectTrial, objectTrial &lt; 9), fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + stat_summary(data = subset(summary_objectTrial, objectTrial &lt; 13 &amp; objectTrial &gt; 8), fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + stat_summary(data = subset(summary_objectTrial, objectTrial &gt; 12), fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + # assigning scico palette tokyo as a color palette scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.75, end=0.2, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;)) + # line showing 0.5 neutral relative score for reference geom_hline(yintercept = 0.5, linetype = 2) + # adding a vertical lines showing the start of a new block with a proper label geom_vline(xintercept = 4.9, alpha = 0.7, linetype=3, size=0.5) + annotate(&quot;text&quot;, label = &quot;Block 2&quot;, x = 5.6, y = 1, size = 3) + geom_vline(xintercept = 8.9, alpha = 0.7, linetype=3, size=0.5) + annotate(&quot;text&quot;, label = &quot;Block 3&quot;, x = 9.6, y = 1, size = 3) + geom_vline(xintercept = 12.9, alpha = 0.7, linetype=3, size=0.5) + annotate(&quot;text&quot;, label = &quot;Block 4&quot;, x = 13.6, y = 1, size = 3) + # changing labels and titles labs(x= &quot;Miniblock&quot;, y = &quot;Relative Influence (0-1)&quot;, subtitle= &quot;Separately per Block&quot;, title=&quot;Positional Memory&quot;, color = &quot;Cue&quot;) + # changing limits and breaks and its labels of both axis scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), limits = c(0,1)) + scale_x_continuous(breaks = c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), labels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)) + # aesthetical changes theme_cowplot() #show the graph g_objTrial The two graphs created above are composed into a single layout for better, more wholesome visualisation. # specifying the layout layout &lt;- &quot; AAAAAA AAAAAA AAAAAA BB#### BB#C## BB#### &quot; # assigning a variable for legend g_leg &lt;- guide_area() g_min &lt;- g_objTrial + g_mini + g_leg + # indicating the layout and gathering the legends plot_layout(design = layout, guides = &quot;collect&quot;) &amp; # unifying aesthetical aspects, mainly text size and style theme(axis.title = element_text(size = 10), axis.text = element_text(size=10), legend.title = element_text(size=10), legend.text = element_text(size=10), plot.title = element_text(size=12, face=&quot;italic&quot;), plot.tag = element_text(size = 10, face=&quot;bold&quot;)) &amp; # adding title and tags plot_annotation(title = &#39;Positional Memory&#39;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = list(c(&#39;A&#39;, &#39;B&#39;))) # show the assembled graphs g_min 4.7.2 Miniblock 1 We noticed that there is a tendency for relative influence score to be above regardless cue-dependency so we wanted to test this statistically so we ran a one-sided, one sample t-test against 0.5. # summarizing by ID for all miniblocks 1 (in block 2-4) sub_mini_1 &lt;- subset_RI %&gt;% filter(miniblock == 1) %&gt;% group_by(ID) %&gt;% summarise(relInfluence = mean(relativeInfluence), .groups =&quot;drop&quot;) # one-tailed, one sample t-test mini1_stats &lt;- t.test(sub_mini_1$relInfluence, mu=0.5, alternative = &quot;greater&quot;) %&gt;% tidy() d &lt;- cohen.d(sub_mini_1$relInfluence, f=NA, mu=0.5) mini1_stats$d &lt;- d$estimate mini1_stats$dCI_low &lt;- d$conf.int[[1]] mini1_stats$dCI_high &lt;- d$conf.int[[2]] Miniblock 1 RI against 0.5 t-test results: t(35)= 6.12, p&lt;0.001, d=1.02, 95% CI [0.3, 1.74] As it seems the relative influence in miniblock is indeed higher than 0.5, therefore they are more likely following boundary as a cue (its old location). Next, we wanted to ensure that there is indeed no difference between the relative influence score between landmark-dependent and boundary-dependent objects in miniblock 1. # summarizing by ID and cue for all miniblocks 1 (in blocks 2-4) sub_mini_cue &lt;- subset_RI %&gt;% filter(miniblock == 1) %&gt;% group_by(ID, cue) %&gt;% summarise(relInfluence = mean(relativeInfluence), .groups =&quot;drop&quot;) # two-tailed, paired sample t-test mini1_cue_stats &lt;- t.test(subset(sub_mini_cue, cue==&quot;landmark&quot;)$relInfluence, subset(sub_mini_cue, cue==&quot;boundary&quot;)$relInfluence, paired=TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(sub_mini_cue, cue==&quot;landmark&quot;)$relInfluence, subset(sub_mini_cue, cue==&quot;boundary&quot;)$relInfluence, paired=TRUE) mini1_cue_stats$d &lt;- d$estimate mini1_cue_stats$dCI_low &lt;- d$conf.int[[1]] mini1_cue_stats$dCI_high &lt;- d$conf.int[[2]] Miniblock 1 landmark-dependent objects’ RI against boundary-dependent objects’ RI t-test results: t(35)= -0.48, p= 0.632, d=-0.12, 95% CI [-0.61, 0.37] HOWEVER! #one sample t-test comparing miniblock 1 relative influence scores to 0.5 for landmark objects mini1_landmark_stats &lt;- t.test(subset(sub_mini_cue, cue==&quot;landmark&quot;)$relInfluence, mu=0.5) %&gt;% tidy() d &lt;- cohen.d(subset(sub_mini_cue, cue==&quot;landmark&quot;)$relInfluence, f=NA, mu=0.5) mini1_landmark_stats$d &lt;- d$estimate mini1_landmark_stats$dCI_low &lt;- d$conf.int[[1]] mini1_landmark_stats$dCI_high &lt;- d$conf.int[[2]] t(35)= 3.78, p&lt;0.001, d=0.63, 95% CI [-0.06, 1.32] #one sample t-test comparing miniblock 1 relative influence scores to 0.5 for boundary objects mini1_boundary_stats &lt;- t.test(subset(sub_mini_cue, cue==&quot;boundary&quot;)$relInfluence, mu=0.5) %&gt;% tidy() d &lt;- cohen.d(subset(sub_mini_cue, cue==&quot;boundary&quot;)$relInfluence, f=NA, mu=0.5) mini1_boundary_stats$d &lt;- d$estimate mini1_boundary_stats$dCI_low &lt;- d$conf.int[[1]] mini1_boundary_stats$dCI_high &lt;- d$conf.int[[2]] t(35)= 4.39, p&lt;0.001, d=0.73, 95% CI [0.03, 1.43] 4.7.3 Mixed Models Relative Influence To test the learning throughout a block, we ran mixed effect model with interaction between cue and miniblock (centered) and added interaction between cue and miniblock as a random slope. formulaMiniblocksCue &lt;- &quot;relativeInfluence ~ cueMM*mini + (1 + cueMM : mini | ID)&quot; modelMiniblocksCue &lt;- lme4::lmer(formula = formulaMiniblocksCue, data=subset_RI) tab_model(modelMiniblocksCue, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;, &quot;Miniblocks&quot;, &quot;Interaction&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.5110 0.0070 0.4973 – 0.5248 Cue 0.1273 0.0048 0.1179 – 0.1368 Miniblocks -0.0266 0.0043 -0.0350 – -0.0181 Interaction 0.0580 0.0052 0.0479 – 0.0681 Random Effects σ2 0.0394 τ00 ID 0.0009 τ11 ID.cueMM:mini 0.0003 ρ01 ID 0.0785 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.344 / 0.365 To test the significance of the interaction, we ran a likelihood ratio test comparing our model and a model with both cue and miniblock as predictors but without the interaction. #control model for interaction formulaMiniblocksControl &lt;- &quot;relativeInfluence ~ cueMM+mini + (1 + cueMM : mini | ID)&quot; modelMiniblocksControl &lt;- lme4::lmer(formula = formulaMiniblocksControl, data=subset_RI) #likelihood ratio test ratioMini &lt;- anova(modelMiniblocksCue, modelMiniblocksControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 54.565, p&lt;0.001 Cue specific analysis To check whether miniblocks are a significant predictor without the cue we ran a model and likelihood ratio test separately for landmark-dependent and boundary-dependent objects. #landmark-dependent objects formulaMiniblocks &lt;- &quot;relativeInfluence ~ mini + (1+mini|ID)&quot; modelMiniblocksLandmark &lt;- lme4::lmer(formula = formulaMiniblocks, data=subset(subset_RI, cue==&quot;landmark&quot;)) #control model formulaMiniControl &lt;- &quot;relativeInfluence ~ 1 + (1+mini|ID)&quot; modelMiniLandmarkControl &lt;- lme4::lmer(formula = formulaMiniControl, data=subset(subset_RI, cue==&quot;landmark&quot;)) #likelihood ratio test ratioMiniLandmark &lt;- anova(modelMiniblocksLandmark, modelMiniLandmarkControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 58.923, p&lt;0.001 # boundary-dependent objects formulaMiniblocks &lt;- &quot;relativeInfluence ~ mini + (1+mini|ID)&quot; modelMiniblocksBoundary &lt;- lme4::lmer(formula = formulaMiniblocks, data=subset(subset_RI,cue==&quot;boundary&quot;)) #control model formulaMiniControl &lt;- &quot;relativeAngle ~ 1 + (1+mini|ID)&quot; modelMiniBoundaryControl &lt;- lme4::lmer(formula = formulaMiniControl, data=subset(subset_RI, cue==&quot;boundary&quot;)) #likelihood ratio test ratioMiniBoundary &lt;- anova(modelMiniblocksBoundary, modelMiniBoundaryControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 554.007, p&lt;0.001 #cue specific models table tab_model(modelMiniblocksLandmark, modelMiniblocksBoundary, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Miniblock&quot;), dv.labels = c(&quot;Landmark-dependent&quot;, &quot;Boundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Landmark-dependent Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.3834 0.0141 0.3558 – 0.4110 0.6377 0.0136 0.6110 – 0.6643 Miniblock -0.0844 0.0070 -0.0981 – -0.0707 0.0321 0.0059 0.0205 – 0.0437 Random Effects σ2 0.0377 0.0323 τ00 0.0055 ID 0.0053 ID τ11 0.0005 ID.mini 0.0001 ID.mini ρ01 0.2725 ID -0.4580 ID N 36 ID 36 ID Observations 851 840 Marginal R2 / Conditional R2 0.169 / 0.286 0.033 / 0.173 Cue Dissonance As we decided to recode relative influence score to dCorrect / (dCorrect + dOther), we ran the main model with miniblocks again with the new dependent variable to see if the slope differ despite the same direction of improvement. Random effects could not included interaction as this caused singular fit. #model with new recoded dependent variable - cue dissonance formulaCueDis_mini &lt;- &quot;cueDissonance ~ cueMM*mini + (1+cueMM+mini|ID)&quot; modelCueDis_mini &lt;- lme4::lmer(formula = formulaCueDis_mini, data=subset_RI) tab_model(modelCueDis_mini, show.p = FALSE, show.se = TRUE, show.icc = FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;, &quot;Miniblock&quot;, &quot;Interaction&quot;), dv.labels = &quot;Cue Dissonance&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Cue Dissonance Predictors Estimates SE Conf. Int (95%) Intercept 0.3728 0.0119 0.3494 – 0.3962 Cue -0.0106 0.0070 -0.0243 – 0.0031 Miniblock -0.0582 0.0052 -0.0684 – -0.0480 Interaction 0.0262 0.0041 0.0183 – 0.0342 Random Effects σ2 0.0349 τ00 ID 0.0044 τ11 ID.cueMM 0.0010 τ11 ID.mini 0.0004 ρ01 -0.0640 0.2528 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.114 / 0.241 To test the significance of the interaction, we ran a likelihood ratio test comparing our model and a model with both cue and miniblock as predictors but without the interaction. #control model for interaction formulaCueDis_miniControl &lt;- &quot;cueDissonance ~ cueMM+mini + (1+cueMM+mini|ID)&quot; modelCueDis_miniControl &lt;- lme4::lmer(formula = formulaCueDis_miniControl, data=subset_RI) #likelihood ratio test ratioCueDis_mini &lt;- anova(modelCueDis_mini, modelCueDis_miniControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 41.109, p&lt;0.001 Graphs CueDis_miniblock &lt;- ggplot(summary_miniblock, aes(miniblock, cueDis, color=cue)) + theme_cowplot() + geom_point(data=subset(summary_miniblock, cue==&quot;boundary&quot;), aes(x = as.numeric(miniblock)-.07), colour = &#39;#845777&#39;, position = position_jitter(width=0.05), alpha=0.7) + geom_point(data=subset(summary_miniblock, cue==&quot;landmark&quot;), aes(x = as.numeric(miniblock)+.07), colour = &#39;#99C095&#39;, position = position_jitter(width=0.05), alpha=0.7) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.8, end=0.2) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + stat_summary(fun=mean, data=subset(summary_miniblock, cue==&quot;boundary&quot;), aes(group=cue), geom=&quot;point&quot;, size = 2, position = position_nudge(x=-.02)) + stat_summary(fun.data=mean_se, data=subset(summary_miniblock, cue==&quot;boundary&quot;), aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95, position = position_nudge(x=-.02)) + stat_summary(fun=mean, data=subset(summary_miniblock, cue==&quot;landmark&quot;), aes(group=cue), geom=&quot;point&quot;, size = 2, position = position_nudge(x=+.02)) + stat_summary(fun.data=mean_se, data=subset(summary_miniblock, cue==&quot;landmark&quot;), aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95, position = position_nudge(x=+.02)) + scale_x_continuous(breaks = c(1,2,3,4)) + scale_y_continuous(limits = c(0.1, 0.9), breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)) + labs(x= &quot;Miniblock&quot;, y = &quot;Cue Dissonance (0-1)&quot;, title=&quot;Positional Memory&quot;, subtitle = &quot;Data&quot;, color=&quot;Cue&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) + theme(legend.position = &quot;none&quot;) CueDis_miniblock The graph below visualizes the cue dissonance miniblock-dependent improvement based on a mixed model predictions. CueDis_predict &lt;- ggeffects::ggpredict(modelCueDis_mini, terms = c(&quot;mini&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) CueDis_miniModel &lt;- ggplot(CueDis_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = sort(unique(Sum_all$mini)), labels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)) + scale_y_continuous(breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9), limits = c(0.1, 0.9)) + theme_cowplot() + labs(x=&quot;Miniblock&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;, title=&quot; &quot;) + theme(legend.position = &quot;none&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) CueDis_miniModel Explorative Follow up t-tests #miniblock 1 m1_cueDis &lt;- t.test(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==1)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==1)$cueDis, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==1)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==1)$cueDis, paired = TRUE) m1_cueDis$d &lt;- d$estimate m1_cueDis$dCI_low &lt;- d$conf.int[[1]] m1_cueDis$dCI_high &lt;- d$conf.int[[2]] #miniblock 2 m2_cueDis &lt;- t.test(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==2)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==2)$cueDis, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==2)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==2)$cueDis, paired = TRUE) m2_cueDis$d &lt;- d$estimate m2_cueDis$dCI_low &lt;- d$conf.int[[1]] m2_cueDis$dCI_high &lt;- d$conf.int[[2]] #miniblock 3 m3_cueDis &lt;- t.test(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==3)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==3)$cueDis, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==3)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==3)$cueDis, paired = TRUE) m3_cueDis$d &lt;- d$estimate m3_cueDis$dCI_low &lt;- d$conf.int[[1]] m3_cueDis$dCI_high &lt;- d$conf.int[[2]] #miniblock 4 m4_cueDis &lt;- t.test(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==4)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==4)$cueDis, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock, cue==&quot;landmark&quot; &amp; miniblock==4)$cueDis, subset(summary_miniblock, cue==&quot;boundary&quot; &amp; miniblock==4)$cueDis, paired = TRUE) m4_cueDis$d &lt;- d$estimate m4_cueDis$dCI_low &lt;- d$conf.int[[1]] m4_cueDis$dCI_high &lt;- d$conf.int[[2]] miniblock 1: t(35)= 5.99, p&lt;0.001, d=1.36, 95% CI [0.73, 1.99] miniblock 2: t(35)= -1.53, p= 0.135, d=-0.25, 95% CI [-0.59, 0.08] miniblock 3: t(35)= -1.17, p= 0.252, d=-0.18, 95% CI [-0.48, 0.13] miniblock 4: t(35)= -1.27, p= 0.212, d=-0.24, 95% CI [-0.62, 0.14] To see whether the model including cue is better than a miniblock alone with random effect we ran a likelihood ratio test. #control model formulaJustMini &lt;- &quot;cueDissonance ~ mini + (1+cueMM+mini|ID)&quot; modelJustMini &lt;- lme4::lmer(formula = formulaJustMini, data=subset_RI) #likelihood ratio test ratioMini &lt;- anova(modelCueDis_miniControl, modelJustMini) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 2.27, p&lt;0.001 4.8 Age To test our main hypothesis, we ran mixed model including age and cue as main predictors with interaction. 4.8.1 Relative Influence Mixed Model As our initial dependent variable, we examine these two predictors (cue, age) first to relative influence scores. #full model formulaFull &lt;- &quot;relativeInfluence ~ age_c*cueMM + ( 1 + cueMM | ID)&quot; modelFull &lt;- lme4::lmer(formula = formulaFull, data=subset_RI, REML = FALSE) #table tab_model(modelFull, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) relativeInfluence Predictors Estimates SE Conf. Int (95%) Intercept 0.5109 0.0069 0.4973 – 0.5245 Age -0.0011 0.0030 -0.0070 – 0.0048 Cue 0.1261 0.0108 0.1050 – 0.1473 Interaction 0.0123 0.0047 0.0031 – 0.0214 Random Effects σ2 0.0407 τ00 ID 0.0009 τ11 ID.cueMM 0.0033 ρ01 ID -0.0588 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.275 / 0.343 The significance of the interaction was tested using likelihood ratio test. #control model controlFinalInt &lt;- &quot;relativeInfluence ~ age_c + cueMM + (1 + cueMM | ID)&quot; modelControl &lt;- lme4::lmer(formula = controlFinalInt, data=subset_RI) #likelihood ratio test ratioFullModel &lt;- anova(modelFull, modelControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 6.279, p= 0.012 Cue specific analysis To see if age remains a significant predictor without cue, we ran mixed models separately for boundary-dependent and landmark-dependent objects and tested them with likelihood ratio test including only random intercepts for participants. #landmark-dependent objects formulaAgeLandmark &lt;- &quot;relativeInfluence ~ age_c + (1|ID)&quot; modelAgeLandmark &lt;- lme4::lmer(formula = formulaAgeLandmark, data=subset(subset_RI, cue==&quot;landmark&quot;)) #control model controlAgeLandmark &lt;- &quot;relativeInfluence ~ 1 + (1 |ID)&quot; modelControlAgeLandmark &lt;- lme4::lmer(formula = controlAgeLandmark, data=subset(subset_RI, cue==&quot;landmark&quot;)) #likelihood ratio test ratioAgeLandmark &lt;- anova(modelAgeLandmark, modelControlAgeLandmark) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 5.227, p= 0.022 #boundary-dependent objects formulaAgeBoundary &lt;- &quot;relativeInfluence ~ age_c +(1|ID)&quot; modelAgeBoundary &lt;- lme4::lmer(formula = formulaAgeBoundary, data=subset(subset_RI, cue==&quot;boundary&quot;)) #control model controlAgeBoundary &lt;- &quot;relativeInfluence ~ 1 + (1 |ID)&quot; modelControlAgeBoundary &lt;- lme4::lmer(formula = controlAgeBoundary, data=subset(subset_RI, cue==&quot;boundary&quot;)) #likelihood ratio test ratioAgeBoundary &lt;- anova(modelAgeBoundary, modelControlAgeBoundary) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 3.912, p= 0.048 tab_model(modelAgeLandmark, modelAgeBoundary, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;), dv.labels = c(&quot;Landmark-dependent&quot;, &quot;Boundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Landmark-dependent Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.3849 0.0134 0.3586 – 0.4111 0.6370 0.0130 0.6116 – 0.6625 Age -0.0134 0.0058 -0.0248 – -0.0020 0.0111 0.0056 0.0001 – 0.0222 Random Effects σ2 0.0475 0.0338 τ00 0.0044 ID 0.0046 ID N 36 ID 36 ID Observations 851 840 Marginal R2 / Conditional R2 0.018 / 0.102 0.017 / 0.135 Graphs The graph below visualizes how the relative influence improves with age as seen in collected data. RI_final &lt;- ggplot(relativeInfluenceBlocks, aes(age, relativeInf, color=cue)) + # setting up color palette - scico tokyo scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.8, end=0.2, guide = FALSE) + # overall mean and se stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size = 2) + stat_summary(fun.data=mean_se, aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95) + # line connecting the avergae values stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + # setting up a new color scale for better visibility of individual geom_points new_scale_color() + # single subject data points (1 per participant) geom_point(aes(color=cue)) + # new color scale - also scico tokyo but in smaller range scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.7, end=0.3, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;)) + # 0.5 neutral score for reference geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) + # chaning breaks and limits of both axis scale_x_continuous(breaks = c(8,9,10,11,12,13,14,15)) + scale_y_continuous(limits = c(0.2, 0.8), breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)) + # changing labels and title labs(x= &quot;Age&quot;, y = &quot;Relative Influence (0-1)&quot;, title= &quot;Positional Memory&quot;, subtitle = &quot;Data&quot;, color=&quot;Cue&quot;) + # background setting theme_cowplot() + theme(legend.position = &quot;none&quot;) # show the graph RI_final The graph below visualizes the relative influence age-dependent improvement based on a mixed model predictions. # calculating values predicted by the mixed model RI_predict &lt;- ggeffects::ggpredict(modelFull, terms = c(&quot;age_c&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) RI_model &lt;- ggplot(RI_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + # plotting the prediction geom_line(size = 0.5) + # plotting the confidence levels geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + # setting up color and fill palette - scico tokyo scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + # changing limits, breaks and labels of both axis scale_x_continuous(breaks = sort(unique(Sum_all$age_c)), labels = c(&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;)) + scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8), limits = c(0.2, 0.8)) + # background setting theme_cowplot() + # changing labels and title labs(x=&quot;Age&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;, title=&quot; &quot;) + # 0.5 neutral score for reference geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) + theme(legend.position = &quot;none&quot;) #show the graph RI_model 4.8.2 Cue dissonance Mixed Model Next, we tested whether the effect of age and age and cue interaction remain when using the new variable - cue dissonance (one direction recoded relative influence score) as dependent variable. formulaCueDis &lt;- &quot;cueDissonance ~ age_c*cueMM + ( 1 + cueMM | ID)&quot; modelCueDis &lt;- lme4::lmer(formula = formulaCueDis, data=subset_RI) tab_model(modelCueDis, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.3739 0.0111 0.3521 – 0.3956 Age -0.0123 0.0048 -0.0217 – -0.0028 Cue -0.0109 0.0071 -0.0248 – 0.0031 Interaction 0.0011 0.0031 -0.0050 – 0.0071 Random Effects σ2 0.0407 τ00 ID 0.0036 τ11 ID.cueMM 0.0010 ρ01 ID -0.0566 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.020 / 0.118 #control model for interaction controlCueDis &lt;- &quot;cueDissonance ~ age_c + cueMM + (1 + cueMM | ID)&quot; modelControlCueDis &lt;- lme4::lmer(formula = controlCueDis, data=subset_RI) #likelihood ratio test ratioCueDis &lt;- anova(modelCueDis, modelControlCueDis) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 0.127, p= 0.722 Explorative Is cue even significant predictor? #control model for cue as predictor controlCueDis_cue &lt;- &quot;cueDissonance ~ age_c + (1 + cueMM | ID)&quot; modelCueDis_cueControl &lt;- lme4::lmer(formula = controlCueDis_cue, data=subset_RI) #likelihood ratio test ratioCueDis_cue &lt;- anova(modelCueDis_cueControl, modelControlCueDis) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 2.349, p= 0.125 Adding miniblock as a predictor instead to find the best model. #control model for miniblock as predictor miniFullCueDis&lt;- &quot;cueDissonance ~ age_c + mini + (1 + cueMM | ID)&quot; modelCueDis_miniFull &lt;- lme4::lmer(formula = miniFullCueDis, data=subset_RI) tab_model(modelCueDis_miniFull, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;, &quot;Miniblock&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.3735 0.0112 0.3516 – 0.3954 Age -0.0121 0.0048 -0.0216 – -0.0026 Miniblock -0.0584 0.0041 -0.0665 – -0.0503 Random Effects σ2 0.0362 τ00 ID 0.0037 τ11 ID.cueMM 0.0011 ρ01 ID -0.0308 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.112 / 0.195 #likelihood ratio test ratioCueDis_miniFull &lt;- anova(modelCueDis_miniFull, modelCueDis_cueControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 187.449, p&lt;0.001 Graphs The graph below visualizes how the cue dissonance improves with age as seen in collected data. CueDis_final &lt;- ggplot(relativeInfluenceBlocks, aes(age, cueDis, color=cue)) + theme_cowplot() + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.8, end=0.2, guide = FALSE) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size = 2) + stat_summary(fun.data=mean_se, aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95) + new_scale_color() + geom_point(aes(color=cue)) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.7, end=0.3, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;)) + scale_x_continuous(breaks = c(8,9,10,11,12,13,14,15)) + scale_y_continuous(limits = c(0.2, 0.8), breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)) + labs(x= &quot;Age&quot;, y = &quot;Cue Dissonance (0-1)&quot;, title=&quot;Positional Memory&quot;, subtitle = &quot;Data&quot;, color=&quot;Cue&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) + theme(legend.position = &quot;none&quot;) CueDis_final The graph below visualizes the cue dissonance age-dependent improvement based on a mixed model predictions. CueDis_predict &lt;- ggeffects::ggpredict(modelCueDis, terms = c(&quot;age_c&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) CueDis_model &lt;- ggplot(CueDis_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = sort(unique(Sum_all$age_c)), labels = c(&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;)) + scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8), limits = c(0.2, 0.8)) + theme_cowplot() + labs(x=&quot;Age&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;, title=&quot; &quot;) + theme(legend.position = &quot;none&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) CueDis_model 4.8.3 Distance error Mixed Model To see if age and cue can predict raw distance error, we ran the same analysis as above but with error in virtual meters as a dependent variable. formulaFullDist &lt;- &quot;error ~ age_c*cueMM + ( 1 + cueMM | ID)&quot; modelFullDist &lt;- lme4::lmer(formula = formulaFullDist, data=subset_RI) summary(modelFullDist) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: error ~ age_c * cueMM + (1 + cueMM | ID) ## Data: subset_RI ## ## REML criterion at convergence: 11436.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.3403 -0.6507 -0.2513 0.4080 4.6841 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 5.861 2.421 ## cueMM 0.757 0.870 0.35 ## Residual 48.092 6.935 ## Number of obs: 1691, groups: ID, 36 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 9.79858 0.43762 22.390 ## age_c -0.74415 0.18984 -3.920 ## cueMM 0.34801 0.22269 1.563 ## age_c:cueMM -0.02019 0.09691 -0.208 ## ## Correlation of Fixed Effects: ## (Intr) age_c cueMM ## age_c -0.031 ## cueMM 0.215 -0.007 ## age_c:cueMM -0.007 0.215 -0.040 tab_model(modelFullDist, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = &quot;Distance Error&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Distance Error Predictors Estimates SE Conf. Int (95%) Intercept 9.7986 0.4376 8.9409 – 10.6563 Age -0.7441 0.1898 -1.1162 – -0.3721 Cue 0.3480 0.2227 -0.0885 – 0.7845 Interaction -0.0202 0.0969 -0.2101 – 0.1697 Random Effects σ2 48.0919 τ00 ID 5.8609 τ11 ID.cueMM 0.7570 ρ01 ID 0.3537 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.052 / 0.167 #control model for interaction controlFullDist&lt;- &quot;error ~ age_c + cueMM + (1 + cueMM | ID)&quot; modelControlFullDist &lt;- lme4::lmer(formula = controlFullDist, data=subset_RI) #likelihood ratio test ratioFullDist &lt;- anova(modelFullDist, modelControlFullDist) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 0.046, p= 0.831 Cue-specific analysis To see if age remains a significant predictor without cue, we ran mixed models separately for boundary-dependent and landmark-dependent objects and tested them with likelihood ratio test including only random intercepts for participants. #landmark-dependent objects formulaAgeLandmark &lt;- &quot;error ~ age_c + (1|ID)&quot; modelAgeLandmark_error &lt;- lme4::lmer(formula = formulaAgeLandmark, data=subset(subset_RI, cue==&quot;landmark&quot;)) #control model controlAgeLandmark &lt;- &quot;error ~ 1 + (1 |ID)&quot; modelControlAgeLandmark_error &lt;- lme4::lmer(formula = controlAgeLandmark, data=subset(subset_RI, cue==&quot;landmark&quot;)) #likelihood ratio test ratioAgeLandmark &lt;- anova(modelAgeLandmark_error, modelControlAgeLandmark_error) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 12.423, p&lt;0.001 #boundary-dependent objects formulaAgeBoundary &lt;- &quot;error ~ age_c +(1|ID)&quot; modelAgeBoundary_error &lt;- lme4::lmer(formula = formulaAgeBoundary, data=subset(subset_RI, cue==&quot;boundary&quot;)) #control model controlAgeBoundary &lt;- &quot;error ~ 1 + (1 |ID)&quot; modelControlAgeBoundary_error &lt;- lme4::lmer(formula = controlAgeBoundary, data=subset(subset_RI, cue==&quot;boundary&quot;)) #likelihood ratio test ratioAgeBoundary &lt;- anova(modelAgeBoundary_error, modelControlAgeBoundary_error) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 10.055, p= 0.002 tab_model(modelAgeLandmark_error, modelAgeBoundary_error, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;), dv.labels = c(&quot;Landmark-dependent&quot;, &quot;Boundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Landmark-dependent Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 9.4494 0.4459 8.5754 – 10.3235 10.1422 0.5311 9.1013 – 11.1832 Age -0.7240 0.1935 -1.1033 – -0.3447 -0.7621 0.2306 -1.2141 – -0.3101 Random Effects σ2 47.0021 49.2024 τ00 5.1582 ID 8.0275 ID N 36 ID 36 ID Observations 851 840 Marginal R2 / Conditional R2 0.050 / 0.144 0.050 / 0.184 Explorative Is cue even significant predictor? #control model for cue as predictor controlFullDist&lt;- &quot;error ~ age_c + (1 + cueMM | ID)&quot; modelControlFullDist &lt;- lme4::lmer(formula = controlFullDist, data=subset_RI) #likelihood ratio test ratioAgeDist &lt;- anova(modelFullDist, modelControlFullDist) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 2.515, p= 0.284 Adding miniblock instead of cue a predictor. #control model for miniblock as predictor miniFullDist&lt;- &quot;error ~ age_c + mini + (1 + cueMM | ID)&quot; modelMiniFullDist &lt;- lme4::lmer(formula = miniFullDist, data=subset_RI) tab_model(modelMiniFullDist, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;, &quot;Miniblock&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 9.6262 0.4287 8.7861 – 10.4664 Age -0.7262 0.1859 -1.0906 – -0.3618 Miniblock -2.1684 0.1409 -2.4446 – -1.8921 Random Effects σ2 41.9717 τ00 ID 6.1288 τ11 ID.cueMM 0.9219 ρ01 ID 0.3634 N ID 36 Observations 1691 Marginal R2 / Conditional R2 0.153 / 0.261 #likelihood ratio test ratioFullMiniDist &lt;- anova(modelMiniFullDist, modelControlFullDist) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 220.937, p&lt;0.001 Graphs distance_final &lt;- ggplot(relativeInfluenceBlocks, aes(age, distanceError, color=cue)) + theme_cowplot() + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, guide = FALSE) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size = 2) + stat_summary(fun.data=mean_se, aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95) + new_scale_color() + geom_point(aes(color=cue)) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.3, end=0.7, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = c(8,9,10,11,12,13,14,15)) + labs(x= &quot;Age&quot;, y = &quot;Distance error (vm)&quot;, subtitle = &quot;Data&quot;, title= &quot;Positional Memory&quot;, color=&quot;Cue&quot;) + theme(legend.position = &quot;none&quot;) distance_final distance_predict &lt;- ggeffects::ggpredict(modelFullDist, terms = c(&quot;age_c&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) distance_model &lt;- ggplot(distance_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = sort(unique(Sum_all$age_c)), labels = c(&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;)) + theme_cowplot() + labs(x=&quot;Age&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;) + theme(legend.position = &quot;none&quot;) distance_model "],["angle-estimation-analysis.html", "5 Angle estimation analysis 5.1 Set up 5.2 Descriptives 5.3 Block 1 Performance 5.4 Cue Differences 5.5 Miniblocks Learning 5.6 Age", " 5 Angle estimation analysis 5.1 Set up knitr::opts_chunk$set(echo = TRUE) Packages used for data analysis library(lme4) library(sjPlot) library(tidyverse) library(broom) library(ggsignif) library(patchwork) library(gghalves) library(cowplot) library(scico) library(Cairo) library(ggnewscale) library(here) library(effsize) library(circular) library(grateful) Functions written for this analysis #function circleFull() generates locations of 100 points that are in shape of circle #used for mapping out the arena in graphs circleFull &lt;- function(center = c(0,0), r = 27.5, npoints = 100){ tt &lt;- seq(0,2*pi,length.out = npoints) xx &lt;- center[1] + r * cos(tt) yy &lt;- center[2] + r * sin(tt) return(data.frame(x = xx, y = yy)) } circleSome &lt;- function(centerX, centerY, r = 27.5, npoints = 100, a1, a2){ tt &lt;- seq(a1,a2,length.out = npoints) xx &lt;- centerX + r * cos(tt) yy &lt;- centerY + r * sin(tt) return(data.frame(x = xx, y = yy)) } Loading in the full dataset Sum_all &lt;- read_delim(here(&quot;data&quot; ,&quot;Sum.txt&quot;), delim = &quot; &quot;, col_names = TRUE, col_types = &quot;fdddddddfdcddddddddddddddddddddddddddddddddddddddd&quot;) Creating a list of subject IDs from the full dataset subjects &lt;- unique(Sum_all$ID) Formatting for results output low_p &lt;- &quot;&lt;0.001&quot; 5.1.1 Exclusion criteria Participants are excluded based on positional memory performance in block 1 which is quantified using memory scores. To be included in the analysis, participant’s memory scores from block 1 need to be significantly greater than the chance level 0.5. exclusion &lt;- c() #creating empty list that will contain the subject IDs that will be excluded for (i_sub in subjects) { score &lt;- t.test(subset(Sum_all, ID==i_sub &amp; block==1)$memoryScoreTraj, mu=0.5, alternative = &quot;greater&quot;) %&gt;% tidy() #one-tailed t.test of block 1 memory scores against chance level 0.5 if (score$p.value &gt; 0.05) { exclusion &lt;- c(exclusion, i_sub) Sum_all &lt;- filter(Sum_all, ID != i_sub) } } number of participants excluded: 2 new overall sample size: 36 5.1.2 Timeout session Identifying and filtering timeout sessions i.e. trials with time over 59 seconds between trial beginning and angle estimation #summary of timeout trials timeout &lt;- filter(Sum_all, secTrialEst &gt; 59) #filtering timeout trials as they do not include location estimation Sum_all &lt;- filter(Sum_all, secTrialEst &lt; 59) number of timeout trials: 5 number of participants with timeout trials: 5 average number of timeout trials per participant with timeout trials: 1 average number of timeout trials per participant: 0.1315789 5.2 Descriptives Reaction Time We looked into reaction time (seconds between end of target object display and angle estimation). #average RT reaction_time_dat &lt;- Sum_all %&gt;% group_by(ID, age) %&gt;% summarise(estResponse = mean(secTrialEst-3.5), .groups = &quot;drop&quot;) #discounting 3.5s for a target display at the beginning of each trial when participants are locked in their position #graph showing distribution, 1 point per participant ggplot(reaction_time_dat, aes(x=0, y=estResponse)) + geom_half_violin(aes(x=-0.05), fill=scico(1, palette = &quot;acton&quot;, begin = 0.45), alpha =0.5, color=NA) + geom_point(aes(x=0.105, color=age), position = position_jitter(width =0.05, height = 0), shape=16, size = 2) + scale_color_scico(palette = &quot;acton&quot;) + geom_boxplot(width = .08, outlier.shape = NA) + theme_cowplot() + ylab(&#39;RT&#39;) + xlab(&#39;&#39;) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), aspect.ratio =1, plot.title = element_text(face=&quot;italic&quot;, size=12)) + scale_y_continuous(limits=c(0, 15)) #actual average RT and sd reaction_time &lt;- summarise(reaction_time_dat, mean = mean(estResponse), sd=sd(estResponse), min=min(estResponse), max=max(estResponse)) The average reaction time in pointing task was 6.601 seconds (sd = 2.514). 5.3 Block 1 Performance This subsection of the analysis aims to answer whether the participants completed the basic task according to the instructions before any manipulation was introduced. Did they understand how to rotate and indicate their angle estimation? 5.3.1 Raw Angle Error Visualization First, let’s look on how participants performed by examining the signed angle error distribution. 0° as “correct angle”. g_anglB1 &lt;- ggplot(Sum_all %&gt;% filter(block == 1), aes(x = angleError)) + # creating histogram to see the distribution geom_histogram(binwidth = 10, boundary=0, fill = scico(1, palette = &quot;lajolla&quot;, begin=0.5), color= &quot;black&quot;, size = .25) + # rounding the histogram to 360° circle coord_polar(start=pi) + # specifying the limits and breaks scale_x_continuous(limits = c(-180,180), breaks = seq(-180, 180, by = 45), minor_breaks = seq(-180, 180, by = 15)) + # background/plot style theme_cowplot() + background_grid() + # changing labels and title labs(x = &quot;Pointing Error (°)&quot;, y = &quot;Count&quot;, title = &quot;Signed Distribution of Error&quot;) + theme(axis.title.y = element_text(hjust = 0.7)) g_anglB1 5.3.2 Angle Error Stats Data summarizing for each participant for block 1. summaryBlock1_Angle &lt;- Sum_all %&gt;% filter(block==1) %&gt;% group_by(ID, block) %&gt;% summarise(angleErr = mean(abs(angleError)), .groups=&quot;drop&quot;) T-Test We took 90° as a threshold of randomness (possible absolute angle error 0-180°). To see whether participants pointed better than on average randomly, we ran a one sample, one-tailed t-test of their average absolute angle error in block 1 against 90. To see the effect size we calculated cohen’s d. b1a_stats &lt;- t.test(summaryBlock1_Angle$angleErr, mu = 90, alternative = &quot;less&quot;) %&gt;% tidy() d &lt;- cohen.d(d=summaryBlock1_Angle$angleErr, f=NA, mu=90) b1a_stats$d &lt;- d$estimate b1a_stats$dCI_low &lt;- d$conf.int[[1]] b1a_stats$dCI_high &lt;- d$conf.int[[2]] Block 1 absolute angle error against 90 t-test results: t(35)= -34.38, p&lt;0.001, d=-5.73, 95% CI [-7.26, -4.2] Graph The graph below shows the average absolute angle error in block 1 for each participant and consequentially the distribution of these values. g_errB1 &lt;- ggplot(summaryBlock1_Angle, aes(x=block, y=angleErr)) + # violin plot geom_half_violin(aes(x=block-0.05), fill=scico(1, palette= &quot;lajolla&quot;, begin = 0.45), alpha =0.5, color=NA) + # single subject data points (1 per participant) with horizontal jitter geom_point(aes(x=block+0.07), position = position_jitter(width =0.02, height = 0), shape=16, size = 1) + # boxplot (median, quartiles) geom_boxplot(width = .05, outlier.shape = NA) + # adding plot of mean and SEM stat_summary(fun = mean, geom = &quot;point&quot;, size=1, shape = 16, position = position_nudge(-.05), colour = &quot;black&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, position = position_nudge(-.05), colour = &quot;black&quot;, width = 0, size = 0.5) + geom_hline(yintercept = 90, linetype=2) + annotate(&quot;text&quot;, label = &quot;90°&quot;, x = 1.3, y = 85, size = 3) + labs(y=&#39;Pointing Error (°)&#39;, x=&#39; &#39;, title = &quot;Average Absolute Error&quot;) + theme_cowplot() + theme(axis.title = element_text(size=10), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.text.y = element_text(size=10)) #show graph g_errB1 Assembly of block 1 graphs for pointing task. angleEst_B1 &lt;- g_errB1 + g_anglB1 &amp; # unifying aesthetics theme(axis.title = element_text(size=12), plot.title = element_text(face=&quot;italic&quot;, size=12)) &amp; # assembly title plot_annotation(title = &#39;Pointing Errors&#39;, theme = theme(plot.title = element_text(size = 14, face=&quot;bold&quot;))) # saving as pdf and png ggsave(file=&quot;AngleErrors_B1.pdf&quot;, plot=angleEst_B1, units = &quot;cm&quot;, width = 15.9, height = 10, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(file=&quot;AngleErrors_B1.png&quot;, plot=angleEst_B1, units = &quot;cm&quot;, width = 15.9, height = 10, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) angleEst_B1 Assembly of block 1 graphs for positional memory and pointing task. block1_graphs &lt;- memory_B1 / angleEst_B1 &amp; # unifying aesthetics theme(axis.title = element_text(size=10), axis.text = element_text(size=10), plot.title = element_text(face=&quot;italic&quot;, size=12), legend.text = element_text(size=10), legend.title = element_text(size=10), plot.tag = element_text(size = 10, face=&quot;bold&quot;)) &amp; # assembly title, subtitle and tags plot_annotation(title = &#39;Block 1 Performance&#39;, subtitle = &#39; \\nPositional Memory&#39;, theme = theme(plot.title = element_text(size = 14, face=&quot;bold&quot;), plot.subtitle = element_text(size=12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) #saving as pdf and png ggsave(file=&quot;B1.pdf&quot;, plot=block1_graphs, units = &quot;cm&quot;, width = 15, height = 16, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(file=&quot;B1.png&quot;, plot=block1_graphs, units = &quot;cm&quot;, width = 15, height = 16, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) block1_graphs 5.4 Cue Differences Next we wanted to investigate whether the participants followed the cues in block 2-4 after landmark and landmark-dependent object movement. Also, is there performance difference between landmark- vs boundary-dependent objects? 5.4.1 Relative Influence To evaluate, which cue participants are using to remember the direction of target object we used relative influence which was created and first used by Doeller, King and Burgess (2008). However, we were the first to use this measurement for angle estimation/pointing. The graph below shows the calculation. Explanation Graph #loading a data from block 2-4 for a single participant graph_sub &lt;- filter(Sum_all, ID == i_sub &amp; block == 2 &amp; trial==5) # generating points that will form arena boundary circle &lt;- circleFull() # generating points that will form angle difference between estimated angle and angle predicted by boundary angle1 &lt;- circleSome(centerX= graph_sub$charX, centerY= graph_sub$charY, r=15, a1= 1.5707963268-rad(graph_sub$estAngle), a2=1.5707963268-rad(graph_sub$boundaryAngle)) # generating points that will form angle difference between estimated angle and angle predicted by landmark angle2 &lt;- circleSome(centerX= graph_sub$charX, centerY= graph_sub$charY, r=10, a1= 1.5707963268-rad(graph_sub$estAngle), a2=1.5707963268-rad(graph_sub$landmarkAngle)) relativeAng_graph &lt;- ggplot(data=graph_sub) + # location predicted by landmark geom_point(aes(x=landmarkCuePosX, y=landmarkCuePosY, color=&quot;Angle Predicted by Landmark&quot;), size = 1.5, alpha=0.3) + # line visualizing the angle from participant&#39;s position to the object location predicted by landmark geom_segment(aes(x=charX, y=charY, xend= landmarkCuePosX, yend=landmarkCuePosY, color=&quot;Angle Predicted by Landmark&quot;), alpha=.9) + # adding aL label annotate(&quot;text&quot;, label=&quot;aL&quot;, x=-3, y=2, size=4, color=&quot;#C6F1B1&quot;, fontface =2) + # location predicted by boundary geom_point(aes(x=boundaryCuePosX, y=boundaryCuePosY, color=&quot;Angle Predicted by Boundary&quot;), size = 1.5, alpha=0.3) + # line visualizing the angle from participant&#39;s position to the object location predicted by boundary geom_segment(aes(x=boundaryCuePosX, y=boundaryCuePosY, xend= charX, yend= charY, color=&quot;Angle Predicted by Boundary&quot;), alpha=.9) + # adding dB label annotate(&quot;text&quot;, label=&quot;aB&quot;, x=-4.5, y=-5, size=4, color=&quot;#592758&quot;, fontface =2) + #participants location geom_point(aes(x=charX, y=charY, color = &quot;Estimated Angle&quot;), size = 2.5) + annotate(&quot;text&quot;, label=&quot;Participant&#39;s location&quot;, x= -12, y = 12, size=3.5, color= &quot;red&quot;) + #landmark location geom_point(aes(x=landmarkX, y=landmarkY), shape = 15, color = &quot;#94A98F&quot;, size=3) + annotate(&quot;text&quot;, label=&quot;Landmark&quot;, x= 15.5, y = 8.9, size=3.5, color= &quot;#94A98F&quot;) + #estimated angle geom_spoke(mapping =aes(x=charX, y=charY, angle = 1.5707963268-rad(estAngle), radius =20), color=&quot;red&quot;, linetype = 1, alpha=0.8) + # angle difference between angle predicted by boundary and estimated angle geom_path(data=angle1, aes(x,y, color=&quot;Angle Predicted by Boundary&quot;))+ # angle difference between angle predicted by landmark and estimated angle geom_path(data=angle2, aes(x,y,color=&quot;Angle Predicted by Landmark&quot;))+ # creating arena boundary geom_path(data=circle, aes(x, y)) + # aesthetical changes theme_cowplot() + theme(aspect.ratio=1, axis.title = element_text(size=10), axis.text = element_text(size=10), plot.title = element_text(size=12), plot.subtitle = element_text(size = 11)) + # changing labels and title labs(title = &quot;Relative Influence Calculation for Angle Estimation&quot;, subtitle = paste(&quot;aL / (aL + aB) &quot;, sprintf(&quot;Angle Relative Influence: %s &quot;, round(graph_sub$relativeAngle, 2)), sep=&quot;\\n&quot;), x= &#39;X (vm)&#39;, y= &#39;Y (vm)&#39;) + # specifying colors manually scale_color_manual(name = &quot; &quot;, values= c(&quot;Angle Predicted by Boundary&quot; = &quot;#592758&quot;, &quot;Angle Predicted by Landmark&quot; = &quot;#C6F1B1&quot;, &quot;Estimated Angle&quot; = &quot;red&quot;)) # saving graph as pdf and png ggsave(filename=&quot;relative_angle_visual.pdf&quot;, plot=relativeAng_graph, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;relative_angle_visual.png&quot;, plot=relativeAng_graph, units = &quot;cm&quot;, width = 15, height = 11, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) # show the graph relativeAng_graph Let’s subset and summarize data for blocks 2-4. subset_RA &lt;- Sum_all %&gt;% filter(block!=1) %&gt;% select(ID, age, age_c, block, miniblock, mini, objectTrial, trial, cue, cueMM, angleError, relativeAngle, cueDissonanceAngle) relativeAngleBlocks &lt;- subset_RA %&gt;% group_by(ID, cue, age) %&gt;% summarise(angleErr = mean(abs(angleError)), relativeAng = mean(relativeAngle), raSD = sd(relativeAngle), cueDisA = mean(cueDissonanceAngle), .groups=&quot;drop&quot;) T-Tests As score 0.5 points towards angle between the angle predicted by landmark and angle predicted by boundary, we ran a one sample, one tail t-test to test whether the relative scores for landmark-dependent objects are less than 0.5. landmarkRA_stats &lt;- t.test(subset(relativeAngleBlocks, cue==&quot;landmark&quot;)$relativeAng, mu = 0.5, alternative = &quot;less&quot;) %&gt;% tidy() d &lt;- cohen.d(d=subset(relativeAngleBlocks, cue==&quot;landmark&quot;)$relativeAng, f=NA, mu = 0.5) landmarkRA_stats$d &lt;- d$estimate landmarkRA_stats$dCI_low &lt;- d$conf.int[[1]] landmarkRA_stats$dCI_high &lt;- d$conf.int[[2]] Landmark-dependent objects’ RA against 0.5 t-test results: t(35)= -7.63, p&lt;0.001, d=-1.27, 95% CI [-2.01, -0.53] After we ran a one sample, one tail t-test to test whether the relative scores for boundary-dependent objects are higher than 0.5. boundaryRA_stats &lt;- t.test(subset(relativeAngleBlocks, cue==&quot;boundary&quot;)$relativeAng, mu = 0.5, alternative = &quot;greater&quot;) %&gt;% tidy() d &lt;- cohen.d(d=subset(relativeAngleBlocks, cue==&quot;boundary&quot;)$relativeAng, f=NA, mu = 0.5) boundaryRA_stats$d &lt;- d$estimate boundaryRA_stats$dCI_low &lt;- d$conf.int[[1]] boundaryRA_stats$dCI_high &lt;- d$conf.int[[2]] Boundary-dependent objects’ RA against 0.5 t-test results: t(35)= 7.87, p&lt;0.001, d=1.31, 95% CI [0.57, 2.06] We also tested these two groups against each other to see if the scores are significantly different. RA_stats &lt;- t.test(subset(relativeAngleBlocks, cue==&quot;boundary&quot;)$relativeAng, subset(relativeAngleBlocks, cue==&quot;landmark&quot;)$relativeAng, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(relativeAngleBlocks, cue==&quot;boundary&quot;)$relativeAng, subset(relativeAngleBlocks, cue==&quot;landmark&quot;)$relativeAng, paired=TRUE) RA_stats$d &lt;- d$estimate RA_stats$dCI_low &lt;- d$conf.int[[1]] RA_stats$dCI_high &lt;- d$conf.int[[2]] Boundary-dependent objects’ RA vs landmark-dependent objects’ RA t-test results: t(35)= 9.38, p&lt;0.001, d=2.55, 95% CI [1.43, 3.67] Graph This difference in relative influence scores between boundary-dependent objects and landmark-dependent objects is visualized in a graph below. rel_InfAng &lt;- ggplot(relativeAngleBlocks, aes(x=cue, y=relativeAng)) + # violin plots gghalves::geom_half_violin(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;r&quot;) + gghalves::geom_half_violin(data=relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;l&quot;) + # scico palette tokyo scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.15, end=0.85) + # single subject data points (1 per participant) geom_point(shape=16, size = 1) + # line connecting individual participants&#39; values geom_line(aes(group=ID), alpha=0.5) + # bowplot distribution geom_boxplot(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.1), width = .1, outlier.shape = NA) + geom_boxplot(data= relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.1), width = .1, outlier.shape = NA) + # adding plot of mean and SEM stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + # background setting theme_cowplot() + # changing axis labels and title labs(x = &quot; &quot;, y = &quot;&quot;, title = &quot;Pointing&quot;) + # visualizing the significance levels of the t-test geom_signif(comparisons = list(c(&quot;landmark&quot;, &quot;boundary&quot;)), test=&quot;t.test&quot;, test.args=list(alternative = &quot;two.sided&quot;, paired=TRUE), map_signif_level = TRUE, tip_length = 0, extend_line = 0.045, y_position = 0.82) + # adding annotation of significance level of individual t-tests annotate(&quot;text&quot;, label=&quot;***&quot;, size=3, y=0.75, x=0.9, fontface =2) + annotate(&quot;text&quot;, label=&quot;***&quot;, size=3, y=0.15, x=2.1, fontface =2) + # adding line at 0.5 (not-following either cue) geom_hline(yintercept=0.5, linetype=2, alpha=0.6) + # axis labels, limits and breaks scale_x_discrete(labels = c(&#39;Boundary&#39;, &#39;Landmark&#39;)) + scale_y_continuous(limits=c(0.1, 0.9), breaks = c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)) ggsave(&quot;relativeInfluenceAng.pdf&quot;, plot=rel_InfAng, units = &quot;cm&quot;, width = 15, height = 12, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;relativeInfluenceAng.png&quot;, plot=rel_InfAng, units = &quot;cm&quot;, width = 15, height = 12, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) rel_InfAng Assembly of relative influence plots for both positional memory and pointing. relative_infl &lt;- rel_Inf + rel_InfAng &amp; theme(axis.title = element_text(size=10), axis.text = element_text(size=10), plot.title = element_text(face=&quot;italic&quot;, size=12), plot.tag = element_text(size = 10, face=&quot;bold&quot;), legend.position = &quot;none&quot;) &amp; plot_annotation(title = &quot;Relative Influence&quot;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) ggsave(&quot;relativeInfluenceAll.pdf&quot;, plot=relative_infl, units = &quot;cm&quot;, width = 15.9, height = 7.3, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;relativeInfluenceAll.png&quot;, plot=relative_infl, units = &quot;cm&quot;, width = 15.9, height = 7.3, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) relative_infl Mixed Model As a first step towards building a full mixed effects model, we tested a simpliest version with only cue as a fixed effect and a random slope and random intercepts for participants. We did not use cue as a factor but in a recoded version of landmark = -1 and boundary = 1. #formula formulaCue &lt;- &quot;relativeAngle ~ cueMM + (1+cueMM|ID)&quot; #model modelCueAngle &lt;- lme4::lmer(formula = formulaCue, data=subset_RA) tab_model(modelCueAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;), dv.labels = c(&quot;Realtive Influence&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Realtive Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.4814 0.0080 0.4656 – 0.4972 Cue 0.1043 0.0111 0.0826 – 0.1261 Random Effects σ2 0.0664 τ00 ID 0.0009 τ11 ID.cueMM 0.0030 ρ01 ID -0.7325 N ID 36 Observations 1704 Marginal R2 / Conditional R2 0.134 / 0.183 To test, whether cue is a significant predictor we ran a likelihood ratio test comparing our simpliest model and a model containing only random effects. #control model without cue as a predictor formulaControl &lt;- &quot;relativeAngle ~ 1 + (1+cueMM|ID)&quot; modelCueControl &lt;- lme4::lmer(formula = formulaControl, data=subset_RA) #likelihood ratio test ratioCueAngle &lt;- anova(modelCueAngle, modelCueControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 45.233, p&lt;0.001 5.4.2 Angle Error Difference Paired, two-tailed t-test to see if the angle error differs between trials with landmark-dependent and boundary-dependent objects. angCue_stats &lt;- t.test(subset(relativeAngleBlocks, cue==&quot;boundary&quot;)$angleErr, subset(relativeAngleBlocks, cue==&quot;landmark&quot;)$angleErr, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(relativeAngleBlocks, cue==&quot;boundary&quot;)$angleErr, subset(relativeAngleBlocks, cue==&quot;landmark&quot;)$angleErr, paired=TRUE) angCue_stats$d &lt;- d$estimate angCue_stats$dCI_low &lt;- d$conf.int[[1]] angCue_stats$dCI_high &lt;- d$conf.int[[2]] Boundary-dependent objects’ angle error vs landmark-dependent objects’ angle error t-test results: t(35)= 0.79, p= 0.434, d=0.08, 95% CI [-0.12, 0.28] Graph below visualizes the lack of difference. cue_Ang &lt;- ggplot(relativeAngleBlocks, aes(x=cue, y= angleErr)) + #violin plots gghalves::geom_half_violin(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;r&quot;) + gghalves::geom_half_violin(data=relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), aes(fill=cue),alpha =0.7, color=NA, side=&quot;l&quot;) + # setting scico palette tokyo as a fill for violin plots scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.15, end=0.85) + # single subject data points (1 per participant) geom_point(shape=16, size = 1) + # line connecting individual participants&#39; values geom_line(aes(group=ID), alpha=0.5) + # boxplots geom_boxplot(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.1), width = .1, outlier.shape = NA) + geom_boxplot(data=relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.1), width = .1, outlier.shape = NA) + # adding plot of mean and SEM stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;landmark&quot;), position=position_nudge(+0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun = mean, geom = &quot;point&quot;, size=1, shape = 16, colour = &quot;black&quot;) + stat_summary(data=relativeAngleBlocks %&gt;% filter(cue==&quot;boundary&quot;), position=position_nudge(-0.2), fun.data = mean_se, geom = &quot;errorbar&quot;, colour = &quot;black&quot;, width = 0, size = 0.5) + # visualizing the significance level of the t-test geom_signif(comparisons = list(c(&quot;landmark&quot;, &quot;boundary&quot;)), test=&quot;t.test&quot;, test.args=list(alternative = &quot;two.sided&quot;, paired=TRUE), map_signif_level = TRUE, tip_length = 0, extend_line = 0.045, y_position = 92, textsize=2.75) + # background setting theme_cowplot() + # changing axis labels and title labs(x = &quot; &quot;, y = &quot;Pointing Error (°)&quot;, title = &quot;Pointing&quot;) + # changing axis limits, breaks and values scale_x_discrete(labels = c(&#39;Boundary&#39;, &#39;Landmark&#39;)) + scale_y_continuous(breaks = c(25, 50, 75, 100), limits = c(9,100)) ggsave(&quot;angleError.pdf&quot;, plot=cue_Ang, units = &quot;cm&quot;, width = 15.9, height = 12, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;angleError.png&quot;, plot=cue_Ang, units = &quot;cm&quot;, width = 15.9, height = 12, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) cue_Ang Assembling plots with raw error for both positional memory and pointing. cue_Diff &lt;- cue_Dist + cue_Ang &amp; theme(axis.title = element_text(size=10), axis.text = element_text(size=10), plot.title = element_text(face=&quot;italic&quot;, size=12), plot.tag = element_text(size = 10, face=&quot;bold&quot;), legend.position = &quot;none&quot;) &amp; plot_annotation(title = &quot;Raw Performance Differences&quot;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) ggsave(&quot;cueDiffAll.pdf&quot;, plot=cue_Diff, units = &quot;cm&quot;, width = 15.9, height = 8.2, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;cueDiffAll.png&quot;, plot=cue_Diff, units = &quot;cm&quot;, width = 15.9, height = 8.2, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) cue_Diff 5.5 Miniblocks Learning 5.5.1 Graphs The graph below shows the relative influence scores averaged for each miniblock in block 2-4 (averaging over 6 scores for each miniblock) summary_miniblock_A &lt;- subset_RA %&gt;% group_by(ID, miniblock, cue, age) %&gt;% summarise(relativeAng = mean(relativeAngle), raSD = sd(relativeAngle), cueDisA = mean(cueDissonanceAngle), .groups = &quot;drop&quot;) g_miniAng &lt;- ggplot(summary_miniblock_A, aes(miniblock, relativeAng, group=interaction(cue, ID), color=cue)) + geom_line(size=0.5, alpha = 0.3) + stat_summary(fun.data=mean_se, aes(group=cue), geom=&quot;errorbar&quot;, width=0.05) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size=1) + geom_hline(yintercept = 0.5, linetype = 2) + theme_cowplot() + labs(x= &quot;Miniblock&quot;, y = &quot;Relative Influence (0-1)&quot;, subtitle = &quot;Averaged across \\nBlocks&quot;, title= &quot; &quot;, color = &quot;Cue&quot;) + theme(axis.title.y = element_blank()) + scale_color_scico_d(palette = &#39;tokyo&#39;,begin=0.75, end=0.2, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;)) + scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), limits = c(0,1)) + scale_x_continuous(limits = c(0.5, 4.5), breaks = c(1,2,3,4)) + theme(legend.position = &quot;none&quot;) g_miniAng The graph below shows the relative influence scores for each miniblock throughout block 2-4 (averaging only over 2 scores per miniblock) #data summarizing summary_objectTrial_A &lt;- subset_RA %&gt;% group_by(ID, objectTrial, cue) %&gt;% summarise(relativeAng = mean(relativeAngle), raSD = sd(relativeAngle), .groups=&quot;drop&quot;) #graph g_objTrialAng &lt;- ggplot(data=summary_objectTrial_A, aes(objectTrial, relativeAng, group=interaction(cue, ID), color=cue)) + theme_cowplot() + geom_line(data= subset(summary_objectTrial_A, objectTrial &lt; 9), size=0.5, alpha = 0.3) + geom_line(data= subset(summary_objectTrial_A, objectTrial &lt; 13 &amp; objectTrial &gt; 8), size=0.5, alpha = 0.3) + geom_line(data= subset(summary_objectTrial_A, objectTrial &gt; 12), size=0.5, alpha = 0.3) + scale_color_scico_d(palette = &#39;tokyo&#39;,begin=0.75, end=0.2, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;)) + labs(x= &quot;Miniblock&quot;, y = &quot;Relative Influence (0-1)&quot;, subtitle= &quot;Separately per Block&quot;, title = &quot;Pointing&quot;, color= &quot;Cue&quot;) + stat_summary(data = subset(summary_objectTrial_A, objectTrial &lt; 9), fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + stat_summary(data = subset(summary_objectTrial_A, objectTrial &lt; 13 &amp; objectTrial &gt; 8), fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + stat_summary(data = subset(summary_objectTrial_A, objectTrial &gt; 12), fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 0.6) + stat_summary(fun.data = mean_se, aes(group=cue), geom = &quot;errorbar&quot;, width=0.05) + stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size=1) + geom_hline(yintercept = 0.5, linetype = 2) + geom_vline(xintercept = 8.9, alpha = 0.7, linetype=3, size=0.5) + geom_vline(xintercept = 12.9, alpha = 0.7, linetype=3, size=0.5) + geom_vline(xintercept = 4.9, alpha = 0.7, linetype=3, size=0.5) + scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), limits = c(0,1)) + scale_x_continuous(breaks = c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), labels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)) + annotate(&quot;text&quot;, label = &quot;Block 2&quot;, x = 5.6, y = 1, size = 3) + annotate(&quot;text&quot;, label = &quot;Block 3&quot;, x = 9.6, y = 1, size = 3) + annotate(&quot;text&quot;, label = &quot;Block 4&quot;, x = 13.6, y = 1, size = 3) + theme(legend.position = &quot;none&quot;) g_objTrialAng 5.5.2 Miniblock 1 We noticed that there is a tendency for relative influence score to be above regardless cue-dependency so we wanted to test this statistically so we ran a one-sided, one sample t-test against 0.5. # summarizing by ID for all miniblocks 1 (in block 2-4) sub_mini_1A &lt;- subset_RA %&gt;% filter(miniblock == 1) %&gt;% group_by(ID) %&gt;% summarise(relAngle = mean(relativeAngle), .groups =&quot;drop&quot;) # one-tailed, one sample t-test mini1a_stats &lt;- t.test(sub_mini_1A$relAngle, mu=0.5, alternative=&quot;greater&quot;) %&gt;% tidy() d &lt;- cohen.d(sub_mini_1A$relAngle, f=NA, mu=0.5) mini1a_stats$d &lt;- d$estimate mini1a_stats$dCI_low &lt;- d$conf.int[[1]] mini1a_stats$dCI_high &lt;- d$conf.int[[2]] Miniblock 1 RA against 0.5 t-test results: t(35)= 2.24, p= 0.016, d=0.37, 95% CI [-0.31, 1.06] As it seems the relative influence in miniblock is indeed higher than 0.5, therefore they are more likely following boundary as a cue (its old location). Next, we wanted to ensure that there is indeed no difference between the relative influence score between landmark-dependent and boundary-dependent objects in miniblock 1. # summarizing by ID and cue for all miniblocks 1 (in blocks 2-4) sub_mini_cueA &lt;- subset_RA %&gt;% filter(miniblock == 1) %&gt;% group_by(ID, cue) %&gt;% summarise(relAngle = mean(relativeAngle), .groups =&quot;drop&quot;) # two-tailed, paired sample t-test mini1a_cue_stats &lt;- t.test(subset(sub_mini_cueA, cue==&quot;landmark&quot;)$relAngle, subset(sub_mini_cueA, cue==&quot;boundary&quot;)$relAngle, paired=TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(sub_mini_cueA, cue==&quot;landmark&quot;)$relAngle, subset(sub_mini_cueA, cue==&quot;boundary&quot;)$relAngle, paired=TRUE) mini1a_cue_stats$d &lt;- d$estimate mini1a_cue_stats$dCI_low &lt;- d$conf.int[[1]] mini1a_cue_stats$dCI_high &lt;- d$conf.int[[2]] Miniblock 1 landmark-dependent objects’ RA against boundary-dependent objects’ RA t-test results: t(35)= -0.34, p= 0.736, d=-0.08, 95% CI [-0.58, 0.41] HOWEVER! #one sample t-test comparing miniblock 1 relative influence scores to 0.5 for landmark objects mini1a_landmark_stats &lt;- t.test(subset(sub_mini_cueA, cue==&quot;landmark&quot;)$relAngle, mu=0.5) %&gt;% tidy() d &lt;- cohen.d(subset(sub_mini_cueA, cue==&quot;landmark&quot;)$relAngle, f=NA, mu=0.5) mini1a_landmark_stats$d &lt;- d$estimate mini1a_landmark_stats$dCI_low &lt;- d$conf.int[[1]] mini1a_landmark_stats$dCI_high &lt;- d$conf.int[[2]] t(35)= 1.18, p= 0.245, d=0.2, 95% CI [-0.48, 0.88] #one sample t-test comparing miniblock 1 relative influence scores to 0.5 for boundary objects mini1a_boundary_stats &lt;- t.test(subset(sub_mini_cueA, cue==&quot;boundary&quot;)$relAngle, mu=0.5) %&gt;% tidy() d &lt;- cohen.d(subset(sub_mini_cueA, cue==&quot;boundary&quot;)$relAngle, f=NA, mu=0.5) mini1a_boundary_stats$d &lt;- d$estimate mini1a_boundary_stats$dCI_low &lt;- d$conf.int[[1]] mini1a_boundary_stats$dCI_high &lt;- d$conf.int[[2]] t(35)= 1.91, p= 0.065, d=0.32, 95% CI [-0.36, 1] 5.5.3 Mixed Models Relative Influence To test the learning throughout a block, we ran mixed effect model with interaction between cue and miniblock (centered) and added interaction between miniblock and cue as a random slope. #formula formulaMiniblocksAngle &lt;- &quot;relativeAngle ~ cueMM * mini + (1 + cueMM : mini | ID)&quot; #model modelMiniblocksAngle &lt;- lme4::lmer(formula = formulaMiniblocksAngle, data=subset_RA) tab_model(modelMiniblocksAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;, &quot;Miniblock&quot;, &quot;Interaction&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.4814 0.0080 0.4658 – 0.4970 Cue 0.1047 0.0062 0.0926 – 0.1169 Miniblock -0.0230 0.0055 -0.0339 – -0.0121 Interaction 0.0478 0.0064 0.0353 – 0.0603 Random Effects σ2 0.0655 τ00 ID 0.0009 τ11 ID.cueMM:mini 0.0004 ρ01 ID -0.0056 N ID 36 Observations 1704 Marginal R2 / Conditional R2 0.178 / 0.195 To test the significance of the interaction, we ran a likelihood ratio test comparing our model and a model with both cue and miniblock as predictors but without the interaction. #control model formulaMiniControl &lt;- &quot;relativeAngle ~ cueMM + mini + ( 1 + cueMM : mini | ID)&quot; modelMiniControlInt &lt;- lme4::lmer(formula = formulaMiniControl, data=subset_RA) #likelihood ratio test ratioMiniInt &lt;- anova(modelMiniblocksAngle, modelMiniControlInt) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 34.27, p&lt;0.001 Cue specific analysis To check whether miniblocks are a significant predictor without the cue we ran a model and likelihood ratio test separately for landmark-dependent and boundary-dependent objects. #landmark-dependent objects only model formulaMiniblocksAngle &lt;- &quot;relativeAngle ~ mini + (1+mini|ID)&quot; modelMiniblocksLandmarkAngle &lt;- lme4::lmer(formula = formulaMiniblocksAngle, data=subset(subset_RA, cue==&quot;landmark&quot;)) #control model formulaMiniControl &lt;- &quot;relativeAngle ~ 1 + (1+mini|ID)&quot; modelMiniLandmarkControl &lt;- lme4::lmer(formula = formulaMiniControl, data=subset(subset_RA, cue==&quot;landmark&quot;)) #likelihood ratio test ratioMiniLandmark &lt;- anova(modelMiniblocksLandmarkAngle, modelMiniLandmarkControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 39.801, p&lt;0.001 !!Singular fit!! #boundary-dependent objects only model formulaMiniblocksAngle &lt;- &quot;relativeAngle ~ mini + ( 1 + mini | ID)&quot; modelMiniblocksBoundaryAngle &lt;- lme4::lmer(formula = formulaMiniblocksAngle, data=subset(subset_RA, cue==&quot;boundary&quot;)) ## boundary (singular) fit: see ?isSingular #control model formulaMiniControl &lt;- &quot;relativeAngle ~ 1 + ( 1 + mini | ID)&quot; modelMiniBoundaryControl &lt;- lme4::lmer(formula = formulaMiniControl, data=subset(subset_RA, cue==&quot;boundary&quot;)) #likelihood ratio test ratioMiniBoundary &lt;- anova(modelMiniblocksBoundaryAngle, modelMiniBoundaryControl) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 9.069, p= 0.003 Cue Dissonance As we decided to recode relative influence score to dCorrect / (dCorrect + dOther), we ran the main model with miniblocks again with the new dependent variable, cue dissonance, to see if the slope differ despite the same direction of improvement. Random effects could not included interaction as this caused singular fit. #model with new recoded dependent variable - cue dissonance formulaCueDis_mini &lt;- &quot;cueDissonanceAngle ~ cueMM*mini + (1+cueMM+mini|ID)&quot; modelCueDis_miniA &lt;- lme4::lmer(formula = formulaCueDis_mini, data=subset_RA) tab_model(modelCueDis_miniA, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;, &quot;Miniblock&quot;, &quot;Interaction&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.3954 0.0111 0.3737 – 0.4171 Cue 0.0186 0.0080 0.0030 – 0.0343 Miniblock -0.0476 0.0064 -0.0601 – -0.0350 Interaction 0.0229 0.0054 0.0123 – 0.0335 Random Effects σ2 0.0623 τ00 ID 0.0031 τ11 ID.cueMM 0.0010 τ11 ID.mini 0.0004 ρ01 -0.6889 -0.0726 N ID 36 Observations 1704 Marginal R2 / Conditional R2 0.054 / 0.119 To test the significance of the interaction, we ran a likelihood ratio test comparing our model and a model with both cue and miniblock as predictors but without the interaction. #control model for interaction formulaCueDis_miniControl &lt;- &quot;cueDissonanceAngle ~ cueMM+mini + (1+cueMM+mini|ID)&quot; modelCueDis_miniControlA &lt;- lme4::lmer(formula = formulaCueDis_miniControl, data=subset_RA) #likelihood ratio test ratioCueDis_miniA &lt;- anova(modelCueDis_miniA, modelCueDis_miniControlA) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 17.776, p&lt;0.001 Graphs CueDis_miniblock_A &lt;- ggplot(summary_miniblock_A, aes(miniblock, cueDisA, color=cue)) + theme_cowplot() + geom_point(data=subset(summary_miniblock_A, cue==&quot;boundary&quot;), aes(x = as.numeric(miniblock)-.07), colour = &#39;#845777&#39;, position = position_jitter(width=0.05), alpha=0.7) + geom_point(data=subset(summary_miniblock_A, cue==&quot;landmark&quot;), aes(x = as.numeric(miniblock)+.07), colour = &#39;#99C095&#39;, position = position_jitter(width=0.05), alpha=0.7) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.8, end=0.2, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;)) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + stat_summary(fun=mean, data=subset(summary_miniblock_A, cue==&quot;boundary&quot;), aes(group=cue), geom=&quot;point&quot;, size = 2, position = position_nudge(x=-.02)) + stat_summary(fun.data=mean_se, data=subset(summary_miniblock_A, cue==&quot;boundary&quot;), aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95, position = position_nudge(x=-.02)) + stat_summary(fun=mean, data=subset(summary_miniblock_A, cue==&quot;landmark&quot;), aes(group=cue), geom=&quot;point&quot;, size = 2, position = position_nudge(x=+.02)) + stat_summary(fun.data=mean_se, data=subset(summary_miniblock_A, cue==&quot;landmark&quot;), aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95, position = position_nudge(x=+.02)) + scale_x_continuous(breaks = c(1,2,3,4)) + scale_y_continuous(limits = c(0.1, 0.9), breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)) + labs(x= &quot;Miniblock&quot;, y = &quot;Cue Dissonance (0-1)&quot;, title=&quot;Pointing&quot;, subtitle = &quot;Data&quot;, color=&quot;Cue&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) + theme(legend.position = &quot;bottom&quot;) CueDis_miniblock_A The graph below visualizes the cue dissonance miniblock-dependent improvement based on a mixed model predictions. CueDis_predict &lt;- ggeffects::ggpredict(modelCueDis_miniA, terms = c(&quot;mini&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) CueDis_miniModel_A &lt;- ggplot(CueDis_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = sort(unique(Sum_all$mini)), labels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;)) + scale_y_continuous(breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9), limits = c(0.1, 0.9)) + theme_cowplot() + labs(x=&quot;Miniblock&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;, title=&quot; &quot;) + theme(legend.position = &quot;bottom&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) CueDis_miniModel_A Explorative Follow up t-tests #miniblock 1 m1_cueDisA &lt;- t.test(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==1)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==1)$cueDisA, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==1)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==1)$cueDisA, paired = TRUE) m1_cueDisA$d &lt;- d$estimate m1_cueDisA$dCI_low &lt;- d$conf.int[[1]] m1_cueDisA$dCI_high &lt;- d$conf.int[[2]] #miniblock 2 m2_cueDisA &lt;- t.test(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==2)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==2)$cueDisA, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==2)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==2)$cueDisA, paired = TRUE) m2_cueDisA$d &lt;- d$estimate m2_cueDisA$dCI_low &lt;- d$conf.int[[1]] m2_cueDisA$dCI_high &lt;- d$conf.int[[2]] #miniblock 3 m3_cueDisA &lt;- t.test(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==3)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==3)$cueDisA, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==3)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==3)$cueDisA, paired = TRUE) m3_cueDisA$d &lt;- d$estimate m3_cueDisA$dCI_low &lt;- d$conf.int[[1]] m3_cueDisA$dCI_high &lt;- d$conf.int[[2]] #miniblock 4 m4_cueDisA &lt;- t.test(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==4)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==4)$cueDisA, paired = TRUE) %&gt;% tidy() d &lt;- cohen.d(subset(summary_miniblock_A, cue==&quot;landmark&quot; &amp; miniblock==4)$cueDisA, subset(summary_miniblock_A, cue==&quot;boundary&quot; &amp; miniblock==4)$cueDisA, paired = TRUE) m4_cueDisA$d &lt;- d$estimate m4_cueDisA$dCI_low &lt;- d$conf.int[[1]] m4_cueDisA$dCI_high &lt;- d$conf.int[[2]] miniblock 1: t(35)= 2.25, p= 0.031, d=0.5, 95% CI [0.03, 0.98] miniblock 2: t(35)= -2.66, p= 0.012, d=-0.52, 95% CI [-0.93, -0.11] miniblock 3: t(35)= -3.79, p&lt;0.001, d=-0.64, 95% CI [-1, -0.27] miniblock 4: t(35)= -3.16, p= 0.003, d=-0.73, 95% CI [-1.25, -0.21] To see whether the model including cue is better than a model only with miniblock as a fixed effect and random effects we ran a likelihood ratio test. #control model formulaJustMini &lt;- &quot;cueDissonanceAngle ~ mini + (1+mini+cueMM|ID)&quot; modelJustMini &lt;- lme4::lmer(formula = formulaJustMini, data=subset_RA) #likelihood ratio test ratioMini &lt;- anova(modelCueDis_miniControlA, modelJustMini) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 5.145, p= 0.023 5.6 Age To test our main hypothesis, we ran mixed model including age and cue as main predictors with interaction. 5.6.1 Relative Influence Mixed Model As our initial dependent variable, we examine these two predictors (cue, age) first to relative influence scores. #full model formulaFullAngle &lt;- &quot;relativeAngle ~ age_c*cueMM+(1+cueMM|ID)&quot; modelFullAngle &lt;- lme4::lmer(formula = formulaFullAngle, data=subset_RA) tab_model(modelFullAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;,&quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Relative Influence Predictors Estimates SE Conf. Int (95%) Intercept 0.4813 0.0082 0.4654 – 0.4973 Age 0.0001 0.0035 -0.0068 – 0.0071 Cue 0.1037 0.0109 0.0824 – 0.1251 Interaction 0.0076 0.0047 -0.0016 – 0.0169 Random Effects σ2 0.0664 τ00 ID 0.0010 τ11 ID.cueMM 0.0029 ρ01 ID -0.7601 N ID 36 Observations 1704 Marginal R2 / Conditional R2 0.138 / 0.185 The significance of the interaction was tested using likelihood ratio test. #control model controlFullAngle &lt;- &quot;relativeAngle ~ age_c+cueMM+(1+cueMM|ID)&quot; modelControlAngle &lt;- lme4::lmer(formula = controlFullAngle, data=subset_RA) #likelihood ratio test ratioFullAngle &lt;- anova(modelFullAngle, modelControlAngle) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 2.671, p= 0.102 Cue specific analysis To see if age remains a significant predictor without cue, we ran mixed models separately for boundary-dependent and landmark-dependent objects and tested them with likelihood ratio test including only random intercepts for participants. #landmark-dependent objects only model formulaAgeLandmark &lt;- &quot;relativeAngle ~ age_c + (1|ID)&quot; modelAgeLandmarkAngle &lt;- lme4::lmer(formula = formulaAgeLandmark, data=subset(subset_RA, cue==&quot;landmark&quot;)) #control model controlAgeLandmark &lt;- &quot;relativeAngle ~ 1 + (1 |ID)&quot; modelControlAgeLandmark &lt;- lme4::lmer(formula = controlAgeLandmark, data=subset(subset_RA, cue==&quot;landmark&quot;)) #likelihood ratio test ratioAgeLandmark &lt;- anova(modelAgeLandmarkAngle, modelControlAgeLandmark) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 1.225, p= 0.268 #boundary-dependent objects only model formulaAgeBoundary &lt;- &quot;relativeAngle ~ age_c + (1|ID)&quot; modelAgeBoundaryAngle &lt;- lme4::lmer(formula = formulaAgeBoundary, data=subset(subset_RA, cue==&quot;boundary&quot;)) #control model controlAgeBoundary &lt;- &quot;relativeAngle ~ 1 + (1 |ID)&quot; modelControlAgeBoundary &lt;- lme4::lmer(formula = controlAgeBoundary, data=subset(subset_RA, cue==&quot;boundary&quot;)) #likelihood ratio test ratioAgeBoundary &lt;- anova(modelAgeBoundaryAngle, modelControlAgeBoundary) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 2.759, p= 0.097 tab_model(modelAgeLandmarkAngle, modelAgeBoundaryAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;), dv.labels = c(&quot;Landmark-dependent&quot;, &quot;Boundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Landmark-dependent Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.3777 0.0160 0.3463 – 0.4090 0.5852 0.0107 0.5642 – 0.6062 Age -0.0075 0.0069 -0.0211 – 0.0061 0.0077 0.0047 -0.0015 – 0.0168 Random Effects σ2 0.0668 0.0660 τ00 0.0064 ID 0.0013 ID N 36 ID 36 ID Observations 855 849 Marginal R2 / Conditional R2 0.004 / 0.091 0.005 / 0.024 Graphs The graph below visualizes how the relative influence improves with age as seen in collected data. RA_final &lt;- ggplot(relativeAngleBlocks, aes(age, relativeAng, color=cue)) + theme_cowplot() + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.8, end=0.2, guide = FALSE) + stat_summary(fun.data = mean_se,geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size = 2) + new_scale_color() + geom_point(data = relativeAngleBlocks, aes(color=cue)) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.7, end=0.3, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;), guide = guide_legend(reverse = TRUE)) + scale_x_continuous(breaks = c(8,9,10,11,12,13,14,15)) + scale_y_continuous(limits = c(0.2, 0.8), breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)) + labs(x= &quot;Age&quot;, y = &quot;Relative Influence (0-1)&quot;, title= &quot;Pointing&quot;, subtitle = &quot;Data&quot;, color=&quot;Cue&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) + theme(legend.position = &quot;bottom&quot;) RA_final The graph below visualizes the relative influence age-dependent improvement based on a mixed model predictions. # calculating values predicted by the mixed model RA_predict &lt;- ggeffects::ggpredict(modelFullAngle, terms = c(&quot;age_c&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) RA_model &lt;- ggplot(RA_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = sort(unique(Sum_all$age_c)), labels = c(&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;)) + scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8), limits = c(0.2, 0.8)) + theme_cowplot() + labs(x=&quot;Age&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;, title= &quot; &quot;) + theme(legend.position = &quot;bottom&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) RA_model 5.6.2 Cue dissonance Mixed Model Next, we tested whether the effect of age and age and cue interaction remain when using the new variable - cue dissonance (one direction recoded relative influence score) as dependent variable. formulaCueDis_A &lt;- &quot;cueDissonanceAngle ~ age_c*cueMM + ( 1 + cueMM | ID)&quot; modelCueDis_A &lt;- lme4::lmer(formula = formulaCueDis_A, data=subset_RA) tab_model(modelCueDis_A, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = &quot;Cue Dissonance&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Cue Dissonance Predictors Estimates SE Conf. Int (95%) Intercept 0.3963 0.0109 0.3749 – 0.4176 Age -0.0076 0.0047 -0.0169 – 0.0016 Cue 0.0187 0.0082 0.0027 – 0.0346 Interaction -0.0001 0.0035 -0.0071 – 0.0068 Random Effects σ2 0.0664 τ00 ID 0.0029 τ11 ID.cueMM 0.0010 ρ01 ID -0.7600 N ID 36 Observations 1704 Marginal R2 / Conditional R2 0.009 / 0.064 #control model for interaction controlCueDis_A &lt;- &quot;cueDissonanceAngle ~ age_c + cueMM + (1 + cueMM | ID)&quot; modelControlCueDis_A &lt;- lme4::lmer(formula = controlCueDis_A, data=subset_RA) #likelihood ratio test ratioCueDis_A &lt;- anova(modelCueDis_A, modelControlCueDis_A) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 0.002, p= 0.965 Explorative Is cue even significant predictor? #control model for cue as predictor controlCueDis_cueA &lt;- &quot;cueDissonanceAngle ~ age_c + (1 + cueMM | ID)&quot; modelCueDis_cueControlA &lt;- lme4::lmer(formula = controlCueDis_cueA, data=subset_RA) summary(modelCueDis_cueControlA) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: cueDissonanceAngle ~ age_c + (1 + cueMM | ID) ## Data: subset_RA ## ## REML criterion at convergence: 285 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.8124 -0.7922 -0.1641 0.6988 2.6874 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 0.002916 0.0540 ## cueMM 0.001211 0.0348 -0.74 ## Residual 0.066402 0.2577 ## Number of obs: 1704, groups: ID, 36 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.406117 0.009990 40.654 ## age_c -0.007651 0.004342 -1.762 ## ## Correlation of Fixed Effects: ## (Intr) ## age_c -0.036 #likelihood ratio test ratioCueDis_cueA &lt;- anova(modelCueDis_cueControlA, modelControlCueDis_A) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 5.148, p= 0.023 Adding miniblock as a predictor instead to find the best model. #control model for miniblock as predictor miniFullCueDis_A&lt;- &quot;cueDissonanceAngle ~ age_c * mini + (1 + cueMM | ID)&quot; modelCueDis_miniFullA &lt;- lme4::lmer(formula = miniFullCueDis_A, data=subset_RA) tab_model(modelCueDis_A, modelCueDis_miniFullA, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = &quot;Relative Influence&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) ## Length of `pred.labels` does not equal number of predictors, no labelling applied. Relative Influence Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) (Intercept) 0.3963 0.0109 0.3749 – 0.4176 0.4061 0.0099 0.3866 – 0.4256 age_c -0.0076 0.0047 -0.0169 – 0.0016 -0.0076 0.0043 -0.0161 – 0.0008 cueMM 0.0187 0.0082 0.0027 – 0.0346 age_c:cueMM -0.0001 0.0035 -0.0071 – 0.0068 mini -0.0473 0.0055 -0.0580 – -0.0366 age_c:mini -0.0044 0.0024 -0.0091 – 0.0002 Random Effects σ2 0.0664 0.0634 τ00 0.0029 ID 0.0029 ID τ11 0.0010 ID.cueMM 0.0013 ID.cueMM ρ01 -0.7600 ID -0.7196 ID N 36 ID 36 ID Observations 1704 1704 Marginal R2 / Conditional R2 0.009 / 0.064 0.047 / 0.090 #control model miniAgeCueDis_Control &lt;- &quot;cueDissonanceAngle ~ age_c + mini + (1 + cueMM | ID)&quot; modelMiniAgeCueDis_Control &lt;- lme4::lmer(formula = miniAgeCueDis_Control, data=subset_RA) #likelihood ratio test ratioCueDis_fullMiniA &lt;- anova(modelCueDis_miniFullA, modelMiniAgeCueDis_Control) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 3.493, p= 0.062 Graphs The graph below visualizes how the cue dissonance improves with age as seen in collected data. CueDisA_final &lt;- ggplot(relativeAngleBlocks, aes(age, cueDisA, color=cue)) + theme_cowplot() + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, guide = FALSE) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size = 2) + stat_summary(fun.data=mean_se, aes(group=cue), geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95) + new_scale_color() + geom_point(aes(color=cue)) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.3, end=0.7, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = c(8,9,10,11,12,13,14,15)) + scale_y_continuous(limits = c(0.2, 0.8), breaks = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)) + labs(x= &quot;Age&quot;, y = &quot;Cue Dissonance (0-1)&quot;, subtitle = &quot;Data&quot;, title=&quot;Pointing&quot;, color=&quot;Cue&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) + theme(legend.position = &quot;bottom&quot;) CueDisA_final The graph below visualizes the cue dissonance age-dependent improvement based on a mixed model predictions. CueDisA_predict &lt;- ggeffects::ggpredict(modelCueDis_A, terms = c(&quot;age_c&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) CueDisA_model &lt;- ggplot(CueDisA_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = sort(unique(Sum_all$age_c)), labels = c(&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;)) + scale_y_continuous(breaks = c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8), limits = c(0.2, 0.8)) + theme_cowplot() + labs(x=&quot;Age&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;, title= &quot; &quot;) + theme(legend.position = &quot;bottom&quot;) + geom_hline(yintercept = 0.5, linetype=2, alpha=0.6) CueDisA_model 5.6.3 Angle error Mixed Model As our last dependent variable, we examine these two predictors (cue, age) to absolute angle error. subset_RA &lt;- mutate(subset_RA, absAngle = abs(angleError)) #full model formulaAngleError &lt;- &quot;absAngle ~ age_c*cueMM+(1+cueMM|ID)&quot; modelAngleError &lt;- lme4::lmer(formula = formulaAngleError, data=subset_RA) tab_model(modelAngleError, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;,&quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = &quot;Angle Error&quot;, string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Angle Error Predictors Estimates SE Conf. Int (95%) Intercept 28.0368 2.2296 23.6668 – 32.4067 Age -2.9734 0.9665 -4.8678 – -1.0790 Cue 0.5892 0.8157 -1.0097 – 2.1880 Interaction 0.1362 0.3548 -0.5592 – 0.8315 Random Effects σ2 885.3825 τ00 ID 160.0614 τ11 ID.cueMM 5.2060 ρ01 ID -0.5617 N ID 36 Observations 1704 Marginal R2 / Conditional R2 0.043 / 0.194 The significance of the interaction was tested using likelihood ratio test. #control model controlAngleError &lt;- &quot;absAngle ~ age_c+cueMM+(1+cueMM|ID)&quot; modelControlAngleError &lt;- lme4::lmer(formula = controlAngleError, data=subset_RA) #likelihood ratio test ratioAngleError &lt;- anova(modelAngleError, modelControlAngleError) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 0.157, p= 0.692 Cue specific analysis To see if age remains a significant predictor without cue, we ran mixed models separately for boundary-dependent and landmark-dependent objects and tested them with likelihood ratio test including only random intercepts for participants. #landmark-dependent objects only model formulaAgeLandmark &lt;- &quot;absAngle ~ age_c + (1|ID)&quot; modelLandmarkAngleError &lt;- lme4::lmer(formula = formulaAgeLandmark, data=subset(subset_RA, cue==&quot;landmark&quot;)) #control model controlAgeLandmark &lt;- &quot;absAngle ~ 1 + (1 |ID)&quot; modelControlLandmarkError &lt;- lme4::lmer(formula = controlAgeLandmark, data=subset(subset_RA, cue==&quot;landmark&quot;)) #likelihood ratio test ratioLandmarkError &lt;- anova(modelLandmarkAngleError, modelControlLandmarkError) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 7.431, p= 0.006 #boundary-dependent objects only model formulaAgeBoundary &lt;- &quot;absAngle ~ age_c + (1|ID)&quot; modelBoundaryAngleError &lt;- lme4::lmer(formula = formulaAgeBoundary, data=subset(subset_RA, cue==&quot;boundary&quot;)) #control model controlAgeBoundary &lt;- &quot;absAngle ~ 1 + (1 |ID)&quot; modelControlBoundaryError &lt;- lme4::lmer(formula = controlAgeBoundary, data=subset(subset_RA, cue==&quot;boundary&quot;)) #likelihood ratio test ratioBoundaryError &lt;- anova(modelBoundaryAngleError, modelControlBoundaryError) %&gt;% tidy() ## refitting model(s) with ML (instead of REML) ## Warning in tidy.anova(.): The following column names in ANOVA output were not ## recognized or transformed: npar \\(\\chi^{2}\\)(1) = 8.547, p= 0.003 tab_model(modelLandmarkAngleError, modelBoundaryAngleError, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;), dv.labels = c(&quot;Landmark-dependent&quot;, &quot;Boundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Landmark-dependent Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 27.4167 2.5584 22.4023 – 32.4310 28.6184 2.1640 24.3770 – 32.8598 Age -3.0963 1.1091 -5.2700 – -0.9226 -2.8301 0.9392 -4.6708 – -0.9893 Random Effects σ2 822.2776 949.1878 τ00 200.7463 ID 128.0753 ID N 36 ID 36 ID Observations 855 849 Marginal R2 / Conditional R2 0.047 / 0.234 0.038 / 0.152 Graphs The graph below visualizes how the angle error improves with age as seen in collected data. AE_final &lt;- ggplot(relativeAngleBlocks, aes(age, angleErr, color=cue)) + theme_cowplot() + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.8, end=0.2, guide = FALSE) + stat_summary(fun.data = mean_se,geom=&quot;errorbar&quot;, size=0.8, width=0.1, alpha=0.95) + stat_summary(fun=mean, aes(group=cue), geom=&quot;line&quot;, size = 1.5) + stat_summary(fun=mean, aes(group=cue), geom=&quot;point&quot;, size = 2) + new_scale_color() + geom_point(data = relativeAngleBlocks, aes(color=cue)) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.7, end=0.3, labels = c(&quot;Landmark&quot;, &quot;Boundary&quot;), guide = guide_legend(reverse = TRUE)) + scale_x_continuous(breaks = c(8,9,10,11,12,13,14,15)) + labs(x= &quot;Age&quot;, y = &quot;Angle Error (°)&quot;, title= &quot;Pointing&quot;, subtitle = &quot;Data&quot;, color=&quot;Cue&quot;) + theme(legend.position = &quot;bottom&quot;) AE_final The graph below visualizes the relative influence age-dependent improvement based on a mixed model predictions. # calculating values predicted by the mixed model AE_predict &lt;- ggeffects::ggpredict(modelAngleError, terms = c(&quot;age_c&quot;, &quot;cueMM&quot;)) %&gt;% as_tibble() %&gt;% mutate(cuePredict = factor(if_else(group == 1, true = &quot;boundary&quot;, false = &quot;landmark&quot;), levels = c(&quot;boundary&quot;, &quot;landmark&quot;))) AE_model &lt;- ggplot(AE_predict, aes(x = x, y = predicted, colour = cuePredict, fill = cuePredict)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2, linetype=0) + geom_line(size = 0.5) + scale_color_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_fill_scico_d(palette = &#39;tokyo&#39;, begin=0.2, end=0.8, labels = c(&quot;Boundary&quot;, &quot;Landmark&quot;)) + scale_x_continuous(breaks = sort(unique(Sum_all$age_c)), labels = c(&quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;, &quot;12&quot;, &quot;13&quot;, &quot;14&quot;, &quot;15&quot;)) + theme_cowplot() + labs(x=&quot;Age&quot;, y=&quot; &quot;, color=&quot;Cue&quot;, fill=&quot;Cue&quot;, subtitle = &quot;Mixed Model&quot;, title= &quot; &quot;) + theme(legend.position = &quot;bottom&quot;) AE_model "],["summary-tables-and-graphs.html", "6 Summary tables and graphs 6.1 Relative Influence Mixed Model 6.2 Miniblocks 6.3 Relative Influence - Miniblock Mixed Model 6.4 Cue Dissonance - Miniblock Mixed Model 6.5 Relative Influence - Age Mixed Model 6.6 Cue Dissonance - Age Mixed Model 6.7 Raw Error - Age Mixed Model", " 6 Summary tables and graphs 6.1 Relative Influence Mixed Model Table tab_model(modelCue, modelCueAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;), dv.labels = c(&quot;Positional Memory&quot;, &quot;Pointing&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory Pointing Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.5108 0.0070 0.4970 – 0.5246 0.4814 0.0080 0.4656 – 0.4972 Cue 0.1270 0.0119 0.1036 – 0.1504 0.1043 0.0111 0.0826 – 0.1261 Random Effects σ2 0.0407 0.0664 τ00 0.0009 ID 0.0009 ID τ11 0.0043 ID.cueMM 0.0030 ID.cueMM ρ01 -0.0876 ID -0.7325 ID N 36 ID 36 ID Observations 1691 1704 Marginal R2 / Conditional R2 0.260 / 0.344 0.134 / 0.183 6.2 Miniblocks Graph Assembling the all four plots for more comprehensive visualization of relative influence improvement over miniblocks. layout &lt;- &quot; AAAAAABB AAAAAABB AAAAAABB CCCCCCDD CCCCCCDD CCCCCCDD EEEEEEEE &quot; g_leg &lt;- guide_area() g_miniblocks &lt;- g_objTrial + g_mini + g_objTrialAng + g_miniAng + g_leg + plot_layout(design = layout, guides = &quot;collect&quot;) &amp; theme(axis.title = element_text(size = 10), axis.text = element_text(size=8), legend.title = element_text(size=10), legend.text = element_text(size=10), legend.direction = &quot;horizontal&quot;, plot.title = element_text(size=12, face = &quot;bold&quot;, vjust = -2), plot.subtitle = element_text(size=11, face=&quot;italic&quot;), plot.tag = element_text(size = 10, face=&quot;bold&quot;, vjust = 4, hjust=-1)) &amp; plot_annotation(title = &#39;Relative Influence Score Improvement over Miniblocks&#39;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) ggsave(&quot;RI_miniblocks.pdf&quot;, plot=g_miniblocks, units = &quot;cm&quot;, width = 15.9, height = 14, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;RI_miniblocks.png&quot;, plot=g_miniblocks, units = &quot;cm&quot;, width = 15.9, height = 14, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) g_miniblocks 6.3 Relative Influence - Miniblock Mixed Model Table Comparison of mixed model for positional memory and pointing. tab_model(modelMiniblocksCue, modelMiniblocksAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;, &quot;Miniblock&quot;, &quot;Interaction&quot;), dv.labels = c(&quot;Positional Memory&quot;, &quot;Pointing&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory Pointing Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.5110 0.0070 0.4973 – 0.5248 0.4814 0.0080 0.4658 – 0.4970 Cue 0.1273 0.0048 0.1179 – 0.1368 0.1047 0.0062 0.0926 – 0.1169 Miniblock -0.0266 0.0043 -0.0350 – -0.0181 -0.0230 0.0055 -0.0339 – -0.0121 Interaction 0.0580 0.0052 0.0479 – 0.0681 0.0478 0.0064 0.0353 – 0.0603 Random Effects σ2 0.0394 0.0655 τ00 0.0009 ID 0.0009 ID τ11 0.0003 ID.cueMM:mini 0.0004 ID.cueMM:mini ρ01 0.0785 ID -0.0056 ID N 36 ID 36 ID Observations 1691 1704 Marginal R2 / Conditional R2 0.344 / 0.365 0.178 / 0.195 Summary with all cue specific mixed models with miniblocks predictors tab_model(modelMiniblocksLandmark, modelMiniblocksBoundary, modelMiniblocksLandmarkAngle, modelMiniblocksBoundaryAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Miniblock&quot;), dv.labels = c(&quot;Positional Memory:\\nLandmark-dependent&quot;, &quot;Positional Memory:\\nBoundary-dependent&quot;, &quot;Pointing:\\nLandmark-dependent&quot;, &quot;Pointing:\\nBoundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory: Landmark-dependent Positional Memory: Boundary-dependent Pointing: Landmark-dependent Pointing: Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.3834 0.0141 0.3558 – 0.4110 0.6377 0.0136 0.6110 – 0.6643 0.3769 0.0159 0.3457 – 0.4081 0.5859 0.0109 0.5645 – 0.6074 Miniblock -0.0844 0.0070 -0.0981 – -0.0707 0.0321 0.0059 0.0205 – 0.0437 -0.0704 0.0084 -0.0868 – -0.0540 0.0248 0.0079 0.0094 – 0.0402 Random Effects σ2 0.0377 0.0323 0.0598 0.0653 τ00 0.0055 ID 0.0053 ID 0.0066 ID 0.0015 ID τ11 0.0005 ID.mini 0.0001 ID.mini 0.0005 ID.mini 0.0000 ID.mini ρ01 0.2725 ID -0.4580 ID -0.1196 ID -1.0000 ID N 36 ID 36 ID 36 ID 36 ID Observations 851 840 855 849 Marginal R2 / Conditional R2 0.169 / 0.286 0.033 / 0.173 0.085 / 0.183 0.011 / 0.034 6.4 Cue Dissonance - Miniblock Mixed Model Table Comparison of mixed model for positional memory and pointing. tab_model(modelCueDis_mini, modelCueDis_miniA, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Cue&quot;, &quot;Miniblock&quot;, &quot;Interaction&quot;), dv.labels = c(&quot;Positional Memory&quot;, &quot;Pointing&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory Pointing Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.3728 0.0119 0.3494 – 0.3962 0.3954 0.0111 0.3737 – 0.4171 Cue -0.0106 0.0070 -0.0243 – 0.0031 0.0186 0.0080 0.0030 – 0.0343 Miniblock -0.0582 0.0052 -0.0684 – -0.0480 -0.0476 0.0064 -0.0601 – -0.0350 Interaction 0.0262 0.0041 0.0183 – 0.0342 0.0229 0.0054 0.0123 – 0.0335 Random Effects σ2 0.0349 0.0623 τ00 0.0044 ID 0.0031 ID τ11 0.0010 ID.cueMM 0.0010 ID.cueMM 0.0004 ID.mini 0.0004 ID.mini ρ01 -0.0640 -0.6889 0.2528 -0.0726 N 36 ID 36 ID Observations 1691 1704 Marginal R2 / Conditional R2 0.114 / 0.241 0.054 / 0.119 Graph layout &lt;- &quot; AAAAA#BBBBBB AAAAA#BBBBBB AAAAA#BBBBBB CCCCC#DDDDDD CCCCC#DDDDDD CCCCC#DDDDDD &quot; g_miniCD &lt;- CueDis_miniblock + CueDis_miniModel + CueDis_miniblock_A + CueDis_miniModel_A + plot_layout(design=layout) &amp; theme(axis.title = element_text(size = 10), axis.text = element_text(size=10), legend.title = element_text(size=10), legend.text = element_text(size=10), legend.direction = &quot;horizontal&quot;, plot.subtitle = element_text(size=12, face=&quot;italic&quot;), plot.title = element_text(size=12, face=&quot;bold&quot;), plot.tag = element_text(size = 10, face=&quot;bold&quot;, vjust = 3, hjust=-1.5)) &amp; plot_annotation(title = &#39;Cue Dissonance Score Improvement with Miniblock&#39;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) ggsave(&quot;cueDissonance_mini.pdf&quot;, plot=g_miniCD, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;cueDissonance_mini.png&quot;, plot=g_miniCD, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) g_miniCD 6.5 Relative Influence - Age Mixed Model Table Overview table with models for positional memory and pointing. tab_model(modelFull, modelFullAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;,&quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = c(&quot;Positional Memory&quot;, &quot;Pointing&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory Pointing Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.5109 0.0069 0.4973 – 0.5245 0.4813 0.0082 0.4654 – 0.4973 Age -0.0011 0.0030 -0.0070 – 0.0048 0.0001 0.0035 -0.0068 – 0.0071 Cue 0.1261 0.0108 0.1050 – 0.1473 0.1037 0.0109 0.0824 – 0.1251 Interaction 0.0123 0.0047 0.0031 – 0.0214 0.0076 0.0047 -0.0016 – 0.0169 Random Effects σ2 0.0407 0.0664 τ00 0.0009 ID 0.0010 ID τ11 0.0033 ID.cueMM 0.0029 ID.cueMM ρ01 -0.0588 ID -0.7601 ID N 36 ID 36 ID Observations 1691 1704 Marginal R2 / Conditional R2 0.275 / 0.343 0.138 / 0.185 Summary with all cue specific mixed models with age as predictor tab_model(modelAgeLandmark, modelAgeBoundary, modelAgeLandmarkAngle, modelAgeBoundaryAngle, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;), dv.labels = c(&quot;Positional Memory:\\nLandmark-dependent&quot;, &quot;Positional Memory:\\nBoundary-dependent&quot;, &quot;Pointing:\\nLandmark-dependent&quot;, &quot;Pointing:\\nBoundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory: Landmark-dependent Positional Memory: Boundary-dependent Pointing: Landmark-dependent Pointing: Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.3849 0.0134 0.3586 – 0.4111 0.6370 0.0130 0.6116 – 0.6625 0.3777 0.0160 0.3463 – 0.4090 0.5852 0.0107 0.5642 – 0.6062 Age -0.0134 0.0058 -0.0248 – -0.0020 0.0111 0.0056 0.0001 – 0.0222 -0.0075 0.0069 -0.0211 – 0.0061 0.0077 0.0047 -0.0015 – 0.0168 Random Effects σ2 0.0475 0.0338 0.0668 0.0660 τ00 0.0044 ID 0.0046 ID 0.0064 ID 0.0013 ID N 36 ID 36 ID 36 ID 36 ID Observations 851 840 855 849 Marginal R2 / Conditional R2 0.018 / 0.102 0.017 / 0.135 0.004 / 0.091 0.005 / 0.024 Graph layout &lt;- &quot; AAAAA#BBBBBB AAAAA#BBBBBB AAAAA#BBBBBB CCCCC#DDDDDD CCCCC#DDDDDD CCCCC#DDDDDD &quot; g_final &lt;- RI_final + RI_model + RA_final + RA_model + plot_layout(design=layout) &amp; theme(axis.title = element_text(size = 10), axis.text = element_text(size=10), legend.title = element_text(size=10), legend.text = element_text(size=10), legend.direction = &quot;horizontal&quot;, plot.subtitle = element_text(size=12, face=&quot;italic&quot;), plot.title = element_text(size=12, face=&quot;bold&quot;), plot.tag = element_text(size = 10, face=&quot;bold&quot;, vjust = 3, hjust=-1.5)) &amp; plot_annotation(title = &#39;Relative Influence Score Improvement with Age&#39;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) ggsave(&quot;relativeInfluence.pdf&quot;, plot=g_final, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;relativeInfluence.png&quot;, plot=g_final, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) g_final 6.6 Cue Dissonance - Age Mixed Model Table tab_model(modelCueDis, modelCueDis_A, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;,&quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = c(&quot;Positional Memory&quot;, &quot;Pointing&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory Pointing Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 0.3739 0.0111 0.3521 – 0.3956 0.3963 0.0109 0.3749 – 0.4176 Age -0.0123 0.0048 -0.0217 – -0.0028 -0.0076 0.0047 -0.0169 – 0.0016 Cue -0.0109 0.0071 -0.0248 – 0.0031 0.0187 0.0082 0.0027 – 0.0346 Interaction 0.0011 0.0031 -0.0050 – 0.0071 -0.0001 0.0035 -0.0071 – 0.0068 Random Effects σ2 0.0407 0.0664 τ00 0.0036 ID 0.0029 ID τ11 0.0010 ID.cueMM 0.0010 ID.cueMM ρ01 -0.0566 ID -0.7600 ID N 36 ID 36 ID Observations 1691 1704 Marginal R2 / Conditional R2 0.020 / 0.118 0.009 / 0.064 Graphs layout &lt;- &quot; AAAAA#BBBBBB AAAAA#BBBBBB AAAAA#BBBBBB CCCCC#DDDDDD CCCCC#DDDDDD CCCCC#DDDDDD &quot; g_finalCD &lt;- CueDis_final + CueDis_model + CueDisA_final + CueDisA_model + plot_layout(design=layout) &amp; theme(axis.title = element_text(size = 10), axis.text = element_text(size=10), legend.title = element_text(size=10), legend.text = element_text(size=10), legend.direction = &quot;horizontal&quot;, plot.subtitle = element_text(size=12, face=&quot;italic&quot;), plot.title = element_text(size=12, face=&quot;bold&quot;), plot.tag = element_text(size = 10, face=&quot;bold&quot;, vjust = 3, hjust=-1.5)) &amp; plot_annotation(title = &#39;Cue Dissonance Score Improvement with Age&#39;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) ggsave(&quot;cueDissonance.pdf&quot;, plot=g_finalCD, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;cueDissonance.png&quot;, plot=g_finalCD, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) g_finalCD 6.7 Raw Error - Age Mixed Model Table tab_model(modelFullDist, modelAngleError, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;,&quot;Age&quot;, &quot;Cue&quot;, &quot;Interaction&quot;), dv.labels = c(&quot;Positional Memory&quot;, &quot;Pointing&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory Pointing Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 9.7986 0.4376 8.9409 – 10.6563 28.0368 2.2296 23.6668 – 32.4067 Age -0.7441 0.1898 -1.1162 – -0.3721 -2.9734 0.9665 -4.8678 – -1.0790 Cue 0.3480 0.2227 -0.0885 – 0.7845 0.5892 0.8157 -1.0097 – 2.1880 Interaction -0.0202 0.0969 -0.2101 – 0.1697 0.1362 0.3548 -0.5592 – 0.8315 Random Effects σ2 48.0919 885.3825 τ00 5.8609 ID 160.0614 ID τ11 0.7570 ID.cueMM 5.2060 ID.cueMM ρ01 0.3537 ID -0.5617 ID N 36 ID 36 ID Observations 1691 1704 Marginal R2 / Conditional R2 0.052 / 0.167 0.043 / 0.194 Summary with all cue specific mixed models with age as predictor tab_model(modelAgeLandmark_error, modelAgeBoundary_error, modelLandmarkAngleError, modelBoundaryAngleError, show.p = FALSE, show.se=TRUE, show.icc=FALSE, digits = 4, digits.re = 4, pred.labels = c(&quot;Intercept&quot;, &quot;Age&quot;), dv.labels = c(&quot;Positional Memory:\\nLandmark-dependent&quot;, &quot;Positional Memory:\\nBoundary-dependent&quot;, &quot;Pointing:\\nLandmark-dependent&quot;, &quot;Pointing:\\nBoundary-dependent&quot;), string.ci = &quot;Conf. Int (95%)&quot;, string.se = &quot;SE&quot;, CSS = css_theme(&quot;cells&quot;)) Positional Memory: Landmark-dependent Positional Memory: Boundary-dependent Pointing: Landmark-dependent Pointing: Boundary-dependent Predictors Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Estimates SE Conf. Int (95%) Intercept 9.4494 0.4459 8.5754 – 10.3235 10.1422 0.5311 9.1013 – 11.1832 27.4167 2.5584 22.4023 – 32.4310 28.6184 2.1640 24.3770 – 32.8598 Age -0.7240 0.1935 -1.1033 – -0.3447 -0.7621 0.2306 -1.2141 – -0.3101 -3.0963 1.1091 -5.2700 – -0.9226 -2.8301 0.9392 -4.6708 – -0.9893 Random Effects σ2 47.0021 49.2024 822.2776 949.1878 τ00 5.1582 ID 8.0275 ID 200.7463 ID 128.0753 ID N 36 ID 36 ID 36 ID 36 ID Observations 851 840 855 849 Marginal R2 / Conditional R2 0.050 / 0.144 0.050 / 0.184 0.047 / 0.234 0.038 / 0.152 Graphs layout &lt;- &quot; AAAAA#BBBBBB AAAAA#BBBBBB AAAAA#BBBBBB CCCCC#DDDDDD CCCCC#DDDDDD CCCCC#DDDDDD &quot; g_finalRE &lt;- distance_final + distance_model + AE_final + AE_model + plot_layout(design=layout) &amp; theme(axis.title = element_text(size = 10), axis.text = element_text(size=10), legend.title = element_text(size=10), legend.text = element_text(size=10), legend.direction = &quot;horizontal&quot;, plot.subtitle = element_text(size=12, face=&quot;italic&quot;), plot.title = element_text(size=12, face=&quot;bold&quot;), plot.tag = element_text(size = 10, face=&quot;bold&quot;, vjust = 3, hjust=-1.5)) &amp; plot_annotation(title = &#39;Raw Error Improvement with Age&#39;, theme = theme(plot.title = element_text(size = 12, face=&quot;bold&quot;)), tag_levels = &#39;A&#39;) ggsave(&quot;rawError.pdf&quot;, plot=g_finalRE, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = cairo_pdf, path = here(&quot;figures&quot;)) ggsave(&quot;rawError.png&quot;, plot=g_finalRE, units = &quot;cm&quot;, width = 15.9, height = 18, dpi = &quot;retina&quot;, device = &quot;png&quot;, path = here(&quot;figures&quot;)) g_finalRE "],["credits.html", "7 Credits", " 7 Credits invisible(grateful::cite_packages(all.pkg = FALSE, include.rmd=FALSE, style = &quot;cell&quot;, out.format = &quot;md&quot;)) ## ## ## processing file: refs.Rmd ## | | | 0% | |......................................................................| 100% ## ordinary text without R code ## output file: refs.knit.md ## /Applications/RStudio.app/Contents/MacOS/pandoc/pandoc +RTS -K512m -RTS refs.knit.md --to markdown_strict-yaml_metadata_block --from markdown+autolink_bare_uris+tex_math_single_backslash --output /Users/makova/Desktop/landmark_boundary/Full-analysis-scripts/citations.md --filter /Applications/RStudio.app/Contents/MacOS/pandoc/pandoc-citeproc ## ## Output created: citations.md used_pkgs &lt;-readLines(here(&quot;citations.md&quot;)) ref_line &lt;- which(used_pkgs==&quot;References&quot;) used_pkgs[ref_line] &lt;- &quot;### References {-}&quot; used_pkgs[ref_line+1] &lt;- &quot;&quot; base (R Core Team, 2020) ggeffects (Lüdecke, 2018) reticulate (Ushey et al., 2021) grateful (Rodríguez-Sánchez and Hutchins, 2020) circular (Agostinelli and Lund, 2017) effsize (Torchiano, 2020) here (Müller, 2020) ggnewscale (Campitelli, 2021) Cairo (Urbanek and Horner, 2020) scico (Pedersen and Crameri, 2020) cowplot (Wilke, 2020) gghalves (Tiedemann, 2020) patchwork (Pedersen, 2020) ggsignif (Constantin and Patil, 2021) broom (Robinson et al., 2021) forcats (Wickham, 2021a) stringr (Wickham, 2019) dplyr (Wickham et al., 2021) purrr (Henry and Wickham, 2020) readr (Wickham and Hester, 2020) tidyr (Wickham, 2021b) tibble (Müller and Wickham, 2020) ggplot2 (Wickham, 2016) tidyverse (Wickham et al., 2019) sjPlot (Lüdecke, 2021) lme4 (Bates et al., 2015) Matrix (Bates and Maechler, 2019) References Agostinelli, C., and Lund, U. (2017). R package circular: Circular statistics (version 0.4-93) (CA: Department of Environmental Sciences, Informatics; Statistics, Ca’ Foscari University, Venice, Italy. UL: Department of Statistics, California Polytechnic State University, San Luis Obispo, California, USA). Bates, D., and Maechler, M. (2019). Matrix: Sparse and dense matrix classes and methods. Bates, D., Mächler, M., Bolker, B., and Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software 67, 1–48. Campitelli, E. (2021). Ggnewscale: Multiple fill and colour scales in ’ggplot2’. Constantin, A.-E., and Patil, I. (2021). ggsignif: R package for displaying significance brackets for ’ggplot2’. PsyArxiv. Henry, L., and Wickham, H. (2020). Purrr: Functional programming tools. Lüdecke, D. (2018). Ggeffects: Tidy data frames of marginal effects from regression models. Journal of Open Source Software 3, 772. Lüdecke, D. (2021). SjPlot: Data visualization for statistics in social science. Müller, K. (2020). Here: A simpler way to find your files. Müller, K., and Wickham, H. (2020). Tibble: Simple data frames. Pedersen, T.L. (2020). Patchwork: The composer of plots. Pedersen, T.L., and Crameri, F. (2020). Scico: Colour palettes based on the scientific colour-maps. R Core Team (2020). R: A language and environment for statistical computing (Vienna, Austria: R Foundation for Statistical Computing). Robinson, D., Hayes, A., and Couch, S. (2021). Broom: Convert statistical objects into tidy tibbles. Rodríguez-Sánchez, F., and Hutchins, S.D. (2020). Grateful: Facilitate citation of r packages. Tiedemann, F. (2020). Gghalves: Compose half-half plots using your favourite geoms. Torchiano, M. (2020). Effsize: Efficient effect size computation. Urbanek, S., and Horner, J. (2020). Cairo: R graphics device using cairo graphics library for creating high-quality bitmap (png, jpeg, tiff), vector (pdf, svg, postscript) and display (x11 and win32) output. Ushey, K., Allaire, J., and Tang, Y. (2021). Reticulate: Interface to ’python’. Wickham, H. (2016). Ggplot2: Elegant graphics for data analysis (Springer-Verlag New York). Wickham, H. (2019). Stringr: Simple, consistent wrappers for common string operations. Wickham, H. (2021a). Forcats: Tools for working with categorical variables (factors). Wickham, H. (2021b). Tidyr: Tidy messy data. Wickham, H., and Hester, J. (2020). Readr: Read rectangular text data. Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., et al. (2019). Welcome to the tidyverse. Journal of Open Source Software 4, 1686. Wickham, H., François, R., Henry, L., and Müller, K. (2021). Dplyr: A grammar of data manipulation. Wilke, C.O. (2020). Cowplot: Streamlined plot theme and plot annotations for ’ggplot2’. "]]
